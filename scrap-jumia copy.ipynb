{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests # make HTTP requests to fetch web pages content\n",
    "from bs4 import BeautifulSoup # parse HTML and XML docs for easier data extraction\n",
    "import csv # write scraped data into CSV file\n",
    "import time # introduce delays between requests to avoid server overload\n",
    "import re #regular expression\n",
    "import random # vary time delays to simulate human-like behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent headers for automating requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Webscraping Page: Jumia**\n",
    "\n",
    "_Products to scrape:_  \n",
    "1. Tvs \n",
    "    * Smart\n",
    "    * Digital\n",
    "2. Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request url\n",
    "url1 = 'https://www.jumia.co.ke/televisions/#catalog-listing' # TVs url\n",
    "url2 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing' # Cookers url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check url status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.jumia.co.ke/televisions/#catalog-listing - status: 200\n",
      "url: https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing - status: 200\n",
      "All pages retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sending GET requests to each URL and check response\n",
    "def check_response(urls, headers):\n",
    "    # Output the status code of each response\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response =requests.get(url, headers=headers)\n",
    "            print(f\"url: {response.url} - status: { response.status_code}\")\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "                return 1  # Return 1 if the request fails\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle GET request exceptions (timeout, connection/http errors etc.)\n",
    "            print(f\"An error occured: {e}\")\n",
    "            return 1  # Return 1 if an error occurs during the request\n",
    "        \n",
    "    return 0  # Return 0 if no error\n",
    "\n",
    "# List of urls\n",
    "urls = [url1, url2]\n",
    "responses = check_response(urls, headers=headers) # Check all url GET requests responses\n",
    "\n",
    "# Check result\n",
    "if responses == 0:\n",
    "    print(\"All pages retrieved successfully.\")\n",
    "else:\n",
    "    print(\"Some pages failed to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that the webpages are paginated, there is need to navigatethrough, and retrieve data from each page.\n",
    "The last page in the page numbers is identified, then all pages are iterated.\n",
    "\n",
    "The pagination URL parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get the last page number\n",
    "def get_last_page_number(urls):\n",
    "    \n",
    "    # Parse webpage content with BeautifulSoup\n",
    "    response =requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find link for last page using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url =last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "    \n",
    "        # Extract product name from 'data-gtm-name' attribute\n",
    "        name = product.get('data-gtm-name', \"N/A\")\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available)\n",
    "        old_price = (\n",
    "            product.find('div', class_='old') or\n",
    "            product.find('span', class_='-tal -gy5 -lthr -fs16 -pvxs -ubpt')\n",
    "        )\n",
    "        old_price = old_price.get_text(strip=True) if old_price else \"N/A\"\n",
    "\n",
    "\n",
    "        # Extract discount (if available)\n",
    "        discount = (\n",
    "            product.find('div', class_='bdg _dsct _sm') or\n",
    "            product.find('span', attrs={'data-disc': True})\n",
    "        )\n",
    "        discount = discount.get_text(strip=True) if discount else \"N/A\"\n",
    "                \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars _m _al' or 'stars _s' class)\n",
    "        stars_rating = product.find('div', class_='stars _m _al') or product.find('div', class_='stars _s')\n",
    "        if stars_rating:\n",
    "            rating = stars_rating.get_text(strip=True).split(\" out of \")[0]  # Extract the rating value (e.g., \"3.9\")\n",
    "        else:\n",
    "            rating = \"N/A\"\n",
    "        \n",
    "        # Extract reviews count (from the 'rev' class or 'verified ratings' link)\n",
    "        #reviews = product.find('div', class_='rev')\n",
    "        #if reviews:\n",
    "        #   reviews_count = reviews.get_text(strip=True).split('(')[-1].split(')')[0]  # Extract the review count (e.g., \"798\")\n",
    "        #else:\n",
    "        #   reviews_link = product.find('a', class_='-plxs _more')\n",
    "        #   reviews_count = reviews_link.get_text(strip=True).split('(')[-1].split(')')[0] if reviews_link else \"N/A\"\n",
    "\n",
    "        # Extract reviews count (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        if reviews:\n",
    "        # Use regular expression to find the number inside parentheses\n",
    "            reviews_count = re.search(r'\\((\\d+)\\)', reviews.get_text(strip=True))\n",
    "            reviews_count = reviews_count.group(1) if reviews_count else \"N/A\"\n",
    "        else:\n",
    "            reviews_link = product.find('a', class_='-plxs _more')\n",
    "            reviews_count = re.search(r'\\((\\d+)\\)', reviews_link.get_text(strip=True))\n",
    "            reviews_count = reviews_count.group(1) if reviews_count else \"N/A\"\n",
    "\n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'rating': rating,\n",
    "            'reviews_count': reviews,\n",
    "            'link': link,\n",
    "        })\n",
    "    \n",
    "    return product_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 from https://www.jumia.co.ke/televisions/#catalog-listing...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Scrape the products from the current page\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m products \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_product_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m products_list\u001b[38;5;241m.\u001b[39mextend(products)  \u001b[38;5;66;03m# Add the scraped products to the main list\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Sleep for a random time between requests to avoid overwhelming the server\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 65\u001b[0m, in \u001b[0;36mscrape_product_details\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     reviews_link \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-plxs _more\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     reviews_count \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m((\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mreviews_link\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     66\u001b[0m     reviews_count \u001b[38;5;241m=\u001b[39m reviews_count\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m reviews_count \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Store all the extracted product details\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "# Iterate over all URLs to scrape data and save to a CSV file\n",
    "for url in urls:\n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "        \n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Determine the output CSV file name based on the URL\n",
    "    if url == url1:\n",
    "        csv_filename = 'data/scrapped/jumia_scraped_televisions.csv' \n",
    "    else:\n",
    "        csv_filename = 'data/scrapped/jumia_scraped_cookers.csv'\n",
    "\n",
    "    # Save the scraped product details to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[ \n",
    "            'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'rating', 'reviews_count', 'link'\n",
    "        ])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        for product in products_list:\n",
    "            writer.writerow(product)\n",
    "\n",
    "    print(f\"Scraped {len(products_list)} products from {url} and saved them to '{csv_filename}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
