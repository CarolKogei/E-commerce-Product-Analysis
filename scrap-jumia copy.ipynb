{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webscraping Products\n",
    "Tvs \n",
    "    * Smart\n",
    "    * Digital\n",
    "Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request url\n",
    "url1 = 'https://www.jumia.co.ke/smart-tvs-2282/'\n",
    "url2 = 'https://www.jumia.co.ke/digital-tvs/'\n",
    "url3 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'\n",
    "\n",
    "#list urls\n",
    "urls = [url1, url2, url3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check url status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jumia.co.ke/smart-tvs-2282/ 200\n",
      "https://www.jumia.co.ke/digital-tvs/ 200\n",
      "https://www.jumia.co.ke/home-cooking-appliances-cookers/ 200\n"
     ]
    }
   ],
   "source": [
    "# Sending GET requests to each URL\n",
    "responses = [requests.get(url) for url in urls]\n",
    "\n",
    "# Output the status code of each response\n",
    "for response in responses:\n",
    "    print(response.url, response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 50 products and saved them to 'jumia_scraped_product_details.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "\n",
    "# Function to scrape all product details from the page\n",
    "def scrape_product_details(url):\n",
    "    # Send GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags with class 'core' that contain product information\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'href': link,\n",
    "            'item_name': name,\n",
    "            'prc': price,\n",
    "            'old': old_price,\n",
    "            'bdg_dsct_sm': discount,\n",
    "            'discount': discount,\n",
    "            'item_id': item_id,\n",
    "            'item_brand': item_brand,\n",
    "            'stars_s': stars_s,\n",
    "            'rev': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Scrape product details from the provided URL\n",
    "products = scrape_product_details(url)\n",
    "\n",
    "# Save the product details to a CSV file\n",
    "with open(\"jumia_scraped_product_details.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'href', 'item_name', 'prc', 'old', 'bdg_dsct_sm', 'discount', 'item_id', 'item_brand', 'stars_s', 'rev'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    # Write each product's details as a row\n",
    "    for product in products:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(products)} products and saved them to 'jumia_scraped_product_details.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraped 50 products and saved them to 'jumia_scraped_all_products.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "\n",
    "# Function to scrape all product details from a specific page\n",
    "def scrape_product_details_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags with class 'core' that contain product information\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'href': link,\n",
    "            'item_name': name,\n",
    "            'prc': price,\n",
    "            'old': old_price,\n",
    "            'bdg_dsct_sm': discount,\n",
    "            'discount': discount,\n",
    "            'item_id': item_id,\n",
    "            'item_brand': item_brand,\n",
    "            'stars_s': stars_s,\n",
    "            'rev': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the last page link\n",
    "    last_page_link = soup.find('a', class_='pg', aria_label='Last Page')\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        # Extract the page number from the href\n",
    "        last_page_url = last_page_link['href']\n",
    "        last_page_number = int(last_page_url.split('page=')[1].split('#')[0])\n",
    "        print(f\"Last page detected: {last_page_number}\")  # Print to confirm\n",
    "        return last_page_number\n",
    "    else:\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Scrape product details from all pages\n",
    "def scrape_all_products(base_url):\n",
    "    # Get the total number of pages\n",
    "    last_page_number = get_last_page_number(base_url)\n",
    "    all_products = []\n",
    "\n",
    "    # Iterate over each page and scrape the products\n",
    "    for page in range(1, last_page_number + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{base_url}?page={page}#catalog-listing\"\n",
    "        products_on_page = scrape_product_details_from_page(page_url)\n",
    "        all_products.extend(products_on_page)\n",
    "    \n",
    "    return all_products\n",
    "\n",
    "# Scrape all product details from all pages\n",
    "products = scrape_all_products(base_url)\n",
    "\n",
    "# Save the product details to a CSV file\n",
    "with open(\"jumia_scraped_all_products.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'href', 'item_name', 'prc', 'old', 'bdg_dsct_sm', 'discount', 'item_id', 'item_brand', 'stars_s', 'rev'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    # Write each product's details as a row\n",
    "    for product in products:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(products)} products and saved them to 'jumia_scraped_all_products.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All <a> tags with class 'pg':\n",
      "<a aria-label=\"Page 2\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=2#catalog-listing\">2</a>\n",
      "<a aria-label=\"Page 3\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=3#catalog-listing\">3</a>\n",
      "<a aria-label=\"Next Page\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=2#catalog-listing\"><svg class=\"ic\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><use xlink:href=\"https://www.jumia.co.ke/assets_he/images/i-icons.da6131d9.svg#arrow-right\"></use></svg></a>\n",
      "<a aria-label=\"Last Page\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=50#catalog-listing\"><svg class=\"ic\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><use xlink:href=\"https://www.jumia.co.ke/assets_he/images/i-icons.da6131d9.svg#last-page\"></use></svg></a>\n",
      "Last page link found: <a aria-label=\"Last Page\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=50#catalog-listing\"><svg class=\"ic\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><use xlink:href=\"https://www.jumia.co.ke/assets_he/images/i-icons.da6131d9.svg#last-page\"></use></svg></a>\n",
      "Last page link URL: /home-cooking-appliances-cookers/?page=50#catalog-listing\n",
      "Last page detected: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# User-Agent headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_last_page_number(url):\n",
    "    # Make the request to get the page content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Ensure the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Return 1 if the request failed\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Debug: Let's print out all the <a> tags to check their contents\n",
    "    all_links = soup.find_all('a', class_='pg')  # Find all <a> tags with class 'pg'\n",
    "    print(\"All <a> tags with class 'pg':\")\n",
    "    for link in all_links:\n",
    "        print(link)\n",
    "\n",
    "    # Find the <a> tag with the aria-label 'Last Page' (check its presence)\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    # Debug: Print out the <a> tag for the last page link if found\n",
    "    if last_page_link:\n",
    "        print(f\"Last page link found: {last_page_link}\")\n",
    "    else:\n",
    "        print(\"No 'Last Page' link found.\")\n",
    "\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        # Extract the href value\n",
    "        last_page_url = last_page_link['href']\n",
    "        print(f\"Last page link URL: {last_page_url}\")  # Debug print statement\n",
    "        \n",
    "        # Extract the page number from the URL (after '?page=' and before '#')\n",
    "        try:\n",
    "            # Split the URL to extract the page number\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            last_page_number = int(page_number)  # Convert the extracted page number to integer\n",
    "            print(f\"Last page detected: {last_page_number}\")  # Confirm output\n",
    "            return last_page_number\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "\n",
    "# Testing the function with the base URL\n",
    "base_url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "get_last_page_number(base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraped 2010 products and saved them to 'jumia_scraped_cookers.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        name = product.find('div', class_='name').get_text(strip=True) if product.find('div', class_='name') else product.get('data-gtm-name', \"N/A\")\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else product.get('data-gtm-price', \"N/A\")\n",
    "        discount = product.find('div', class_='bdg _dsct').get_text(strip=True) if product.find('div', class_='bdg _dsct') else \"N/A\"\n",
    "        brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        img_tag = product.find('img')\n",
    "        img_url = img_tag['data-src'] if img_tag and 'data-src' in img_tag.attrs else \"N/A\"\n",
    "        \n",
    "        data_id = product.get('data-gtm-id', \"N/A\")\n",
    "        data_category = product.get('data-gtm-category', \"N/A\")\n",
    "        data_discount = product.get('data-ga4-discount', \"N/A\")\n",
    "        data_price = product.get('data-ga4-price', \"N/A\")\n",
    "        data_name = product.get('data-ga4-item_name', \"N/A\")\n",
    "        \n",
    "        product_details.append({\n",
    "            'link': link,\n",
    "            'name': name,\n",
    "            'price': price,\n",
    "            'discount': discount,\n",
    "            'brand': brand,\n",
    "            'image_url': img_url,\n",
    "            'data_id': data_id,\n",
    "            'data_category': data_category,\n",
    "            'data_discount': data_discount,\n",
    "            'data_price': data_price,\n",
    "            'data_name': data_name\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Base URL for the cookers category\n",
    "base_url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "\n",
    "# Get the last page number\n",
    "last_page = get_last_page_number(base_url)\n",
    "\n",
    "# Initialize an empty list to store all products\n",
    "cookers = []\n",
    "\n",
    "# Iterate through all pages from 1 to the last page\n",
    "for page_num in range(1, last_page + 1):\n",
    "    page_url = f\"{base_url}?page={page_num}#catalog-listing\"\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Scrape the products from the current page\n",
    "    products = scrape_product_details(page_url)\n",
    "    cookers.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "    # Sleep for a random time between requests to avoid overwhelming the server\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# Save the scraped product details to a CSV file\n",
    "with open(\"jumia_scraped_cookers.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        \"link\", \"name\", \"price\", \"discount\", \"brand\", \"image_url\",\n",
    "        \"data_id\", \"data_category\", \"data_discount\", \"data_price\", \"data_name\"\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    for product in cookers:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(cookers)} products and saved them to 'jumia_scraped_cookers.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape SmartTvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3316088077.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[48], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    for itm in range(1,51):\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "url = (f\"https://www.jumia.co.ke/home-cooking-appliances-cookers/?page={x}#catalog-listing\")\n",
    "\n",
    "for itm in range(1,51):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
