{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webscraping Products\n",
    "Tvs \n",
    "    * Smart\n",
    "    * Digital\n",
    "Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request url\n",
    "url1 = 'https://www.jumia.co.ke/smart-tvs-2282/'\n",
    "url2 = 'https://www.jumia.co.ke/digital-tvs/'\n",
    "url3 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'\n",
    "\n",
    "#list urls\n",
    "urls = [url1, url2, url3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check url status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jumia.co.ke/smart-tvs-2282/ 200\n",
      "https://www.jumia.co.ke/digital-tvs/ 200\n",
      "https://www.jumia.co.ke/home-cooking-appliances-cookers/ 200\n"
     ]
    }
   ],
   "source": [
    "# Sending GET requests to each URL\n",
    "responses = [requests.get(url) for url in urls]\n",
    "\n",
    "# Output the status code of each response\n",
    "for response in responses:\n",
    "    print(response.url, response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 49 products and saved them to 'jumia_scraped_product_details.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "\n",
    "# Function to scrape all product details from the page\n",
    "def scrape_product_details(url):\n",
    "    # Send GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags with class 'core' that contain product information\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'href': link,\n",
    "            'item_name': name,\n",
    "            'prc': price,\n",
    "            'old': old_price,\n",
    "            'bdg_dsct_sm': discount,\n",
    "            'discount': discount,\n",
    "            'item_id': item_id,\n",
    "            'item_brand': item_brand,\n",
    "            'stars_s': stars_s,\n",
    "            'rev': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Scrape product details from the provided URL\n",
    "products = scrape_product_details(url)\n",
    "\n",
    "# Save the product details to a CSV file\n",
    "with open(\"jumia_scraped_product_details.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'href', 'item_name', 'prc', 'old', 'bdg_dsct_sm', 'discount', 'item_id', 'item_brand', 'stars_s', 'rev'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    # Write each product's details as a row\n",
    "    for product in products:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(products)} products and saved them to 'jumia_scraped_product_details.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraped 49 products and saved them to 'jumia_scraped_all_products.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "\n",
    "# Function to scrape all product details from a specific page\n",
    "def scrape_product_details_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags with class 'core' that contain product information\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'href': link,\n",
    "            'item_name': name,\n",
    "            'prc': price,\n",
    "            'old': old_price,\n",
    "            'bdg_dsct_sm': discount,\n",
    "            'discount': discount,\n",
    "            'item_id': item_id,\n",
    "            'item_brand': item_brand,\n",
    "            'stars_s': stars_s,\n",
    "            'rev': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the last page link\n",
    "    last_page_link = soup.find('a', class_='pg', aria_label='Last Page')\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        # Extract the page number from the href\n",
    "        last_page_url = last_page_link['href']\n",
    "        last_page_number = int(last_page_url.split('page=')[1].split('#')[0])\n",
    "        print(f\"Last page detected: {last_page_number}\")  # Print to confirm\n",
    "        return last_page_number\n",
    "    else:\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Scrape product details from all pages\n",
    "def scrape_all_products(base_url):\n",
    "    # Get the total number of pages\n",
    "    last_page_number = get_last_page_number(base_url)\n",
    "    all_products = []\n",
    "\n",
    "    # Iterate over each page and scrape the products\n",
    "    for page in range(1, last_page_number + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{base_url}?page={page}#catalog-listing\"\n",
    "        products_on_page = scrape_product_details_from_page(page_url)\n",
    "        all_products.extend(products_on_page)\n",
    "    \n",
    "    return all_products\n",
    "\n",
    "# Scrape all product details from all pages\n",
    "products = scrape_all_products(base_url)\n",
    "\n",
    "# Save the product details to a CSV file\n",
    "with open(\"jumia_scraped_all_products.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'href', 'item_name', 'prc', 'old', 'bdg_dsct_sm', 'discount', 'item_id', 'item_brand', 'stars_s', 'rev'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    # Write each product's details as a row\n",
    "    for product in products:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(products)} products and saved them to 'jumia_scraped_all_products.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All <a> tags with class 'pg':\n",
      "<a aria-label=\"Page 2\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=2#catalog-listing\">2</a>\n",
      "<a aria-label=\"Page 3\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=3#catalog-listing\">3</a>\n",
      "<a aria-label=\"Next Page\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=2#catalog-listing\"><svg class=\"ic\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><use xlink:href=\"https://www.jumia.co.ke/assets_he/images/i-icons.da6131d9.svg#arrow-right\"></use></svg></a>\n",
      "<a aria-label=\"Last Page\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=50#catalog-listing\"><svg class=\"ic\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><use xlink:href=\"https://www.jumia.co.ke/assets_he/images/i-icons.da6131d9.svg#last-page\"></use></svg></a>\n",
      "Last page link found: <a aria-label=\"Last Page\" class=\"pg\" href=\"/home-cooking-appliances-cookers/?page=50#catalog-listing\"><svg class=\"ic\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><use xlink:href=\"https://www.jumia.co.ke/assets_he/images/i-icons.da6131d9.svg#last-page\"></use></svg></a>\n",
      "Last page link URL: /home-cooking-appliances-cookers/?page=50#catalog-listing\n",
      "Last page detected: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# User-Agent headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_last_page_number(url):\n",
    "    # Make the request to get the page content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Ensure the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Return 1 if the request failed\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Debug: Let's print out all the <a> tags to check their contents\n",
    "    all_links = soup.find_all('a', class_='pg')  # Find all <a> tags with class 'pg'\n",
    "    print(\"All <a> tags with class 'pg':\")\n",
    "    for link in all_links:\n",
    "        print(link)\n",
    "\n",
    "    # Find the <a> tag with the aria-label 'Last Page' (check its presence)\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    # Debug: Print out the <a> tag for the last page link if found\n",
    "    if last_page_link:\n",
    "        print(f\"Last page link found: {last_page_link}\")\n",
    "    else:\n",
    "        print(\"No 'Last Page' link found.\")\n",
    "\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        # Extract the href value\n",
    "        last_page_url = last_page_link['href']\n",
    "        print(f\"Last page link URL: {last_page_url}\")  # Debug print statement\n",
    "        \n",
    "        # Extract the page number from the URL (after '?page=' and before '#')\n",
    "        try:\n",
    "            # Split the URL to extract the page number\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            last_page_number = int(page_number)  # Convert the extracted page number to integer\n",
    "            print(f\"Last page detected: {last_page_number}\")  # Confirm output\n",
    "            return last_page_number\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "\n",
    "# Testing the function with the base URL\n",
    "base_url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "get_last_page_number(base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraped 2009 products and saved them to 'jumia_scraped_cookers1.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'link': link,\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'ratings': stars_s,\n",
    "            'reviews_count': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "# Get the last page number\n",
    "last_page = get_last_page_number(url3)\n",
    "\n",
    "# Initialize an empty list to store all products\n",
    "cookers = []\n",
    "\n",
    "# Iterate through all pages from 1 to the last page\n",
    "for page_num in range(1, last_page + 1):\n",
    "    page_url = f\"{url3}?page={page_num}#catalog-listing\"\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Scrape the products from the current page\n",
    "    products = scrape_product_details(page_url)\n",
    "    cookers.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "    # Sleep for a random time between requests to avoid overwhelming the server\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# Save the scraped product details to a CSV file\n",
    "with open(\"jumia_scraped_cookers1.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'link', 'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'ratings', 'reviews_count'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    for product in cookers:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(cookers)} products and saved them to 'jumia_scraped_cookers1.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape SmartTvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests # make HTTP requests to fetch web pages content\n",
    "from bs4 import BeautifulSoup # parse HTML and XML docs for easier data extraction\n",
    "import csv # write scraped data into CSV file\n",
    "import time # introduce delays between requests to avoid server overload\n",
    "import random # vary time delays to simulate human-like behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWedKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_product_details(url):\n",
    "    reponse = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative url)\n",
    "        link = product['href'] if 'href' in product.attrs else None#\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of the Last Page\n",
    "def get_last_page_number(url):\n",
    "    response =requests.get(url, headers=headers)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1 # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link =soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1 # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1 # Default to 1 page if no last page is found\n",
    "\n",
    "last_page = get_last_page_number(url1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate for all page number of product listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 2 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 3 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 4 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 5 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 6 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 7 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 8 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 9 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 10 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 11 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 12 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 13 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 14 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 15 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 16 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 17 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 18 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 19 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 20 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 21 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 22 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 23 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 24 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 25 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 26 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 27 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 28 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 29 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 30 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 31 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 32 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 33 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 34 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 35 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 36 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 37 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 38 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 39 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 40 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 41 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 42 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 43 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 44 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 45 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 46 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 47 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 48 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 49 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraping page 50 from https://www.jumia.co.ke/smart-tvs-2282/...\n",
      "Scraped 2063 products from https://www.jumia.co.ke/smart-tvs-2282/ and saved them to 'jumia_scraped_smart-tvs.csv'.\n",
      "Scraping page 1 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 2 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 3 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 4 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 5 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 6 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 7 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 8 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 9 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 10 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 11 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 12 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 13 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 14 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 15 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 16 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 17 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 18 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 19 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 20 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 21 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 22 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 23 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraping page 24 from https://www.jumia.co.ke/digital-tvs/...\n",
      "Scraped 963 products from https://www.jumia.co.ke/digital-tvs/ and saved them to 'jumia_scraped_digital-tvs.csv'.\n",
      "Scraping page 1 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 2 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 3 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 4 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 5 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 6 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 7 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 8 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 9 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 10 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 11 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 12 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 13 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 14 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 15 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 16 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 17 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 18 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 19 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 20 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 21 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 22 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 23 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 24 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 25 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 26 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 27 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 28 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 29 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 30 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 31 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 32 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 33 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 34 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 35 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 36 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 37 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 38 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 39 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 40 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 41 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 42 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 43 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 44 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 45 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 46 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 47 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 48 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 49 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 50 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraped 2009 products from https://www.jumia.co.ke/home-cooking-appliances-cookers/ and saved them to 'jumia_scraped_cookers.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# List of URLs to scrape\n",
    "url1 = 'https://www.jumia.co.ke/smart-tvs-2282/'\n",
    "url2 = 'https://www.jumia.co.ke/digital-tvs/'\n",
    "url3 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'\n",
    "\n",
    "urls = [url1, url2, url3]\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        # Extract product name from 'data-gtm-name' attribute\n",
    "        name = product.get('data-gtm-name', \"N/A\")  # Extract from the 'data-gtm-name' attribute\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'link': link,\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'ratings': stars_s,\n",
    "            'reviews_count': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Iterate over all URLs to scrape data and save to a CSV file\n",
    "for url in urls:\n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "        \n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Determine the output CSV file name based on the URL\n",
    "    if url == url1:\n",
    "        csv_filename = 'jumia_scraped_smart-tvs.csv'\n",
    "    elif url == url2:\n",
    "        csv_filename = 'jumia_scraped_digital-tvs.csv'\n",
    "    else:\n",
    "        csv_filename = 'jumia_scraped_cookers.csv'\n",
    "\n",
    "    # Save the scraped product details to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\n",
    "            'link', 'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'ratings', 'reviews_count'\n",
    "        ])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        for product in products_list:\n",
    "            writer.writerow(product)\n",
    "\n",
    "    print(f\"Scraped {len(products_list)} products from {url} and saved them to '{csv_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 2 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 3 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 4 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 5 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 6 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 7 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 8 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 9 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 10 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 11 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 12 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 13 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 14 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 15 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 16 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 17 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 18 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 19 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 20 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 21 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 22 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 23 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 24 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 25 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 26 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 27 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 28 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 29 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 30 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 31 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 32 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 33 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 34 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 35 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 36 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 37 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 38 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 39 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 40 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 41 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 42 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 43 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 44 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 45 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 46 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 47 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 48 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 49 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 50 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraped 5050 products from https://www.jumia.co.ke/televisions/#catalog-listing and saved them to 'jumia_scraped_televisions.csv'.\n",
      "Scraping page 1 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 2 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 3 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 4 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 5 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 6 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 7 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 8 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 9 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 10 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 11 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 12 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 13 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 14 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 15 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 16 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 17 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 18 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 19 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 20 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 21 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 22 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 23 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 24 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 25 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 26 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 27 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 28 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 29 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 30 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 31 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 32 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 33 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 34 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 35 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 36 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 37 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 38 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 39 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 40 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 41 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 42 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 43 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 44 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 45 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 46 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 47 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 48 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 49 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraping page 50 from https://www.jumia.co.ke/home-cooking-appliances-cookers/...\n",
      "Scraped 2009 products from https://www.jumia.co.ke/home-cooking-appliances-cookers/ and saved them to 'jumia_scraped_cookers.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# List of URLs to scrape (only one URL for televisions now)\n",
    "url1 = 'https://www.jumia.co.ke/televisions/#catalog-listing'  # Updated to televisions\n",
    "url3 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'  # Remains unchanged\n",
    "\n",
    "urls = [url1, url3]  # Removed the duplicate url2\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        # Extract product name from 'data-gtm-name' attribute\n",
    "        name = product.get('data-gtm-name', \"N/A\")  # Extract from the 'data-gtm-name' attribute\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars').get_text(strip=True) if product.find('div', class_='stars') else \"N/A\"\n",
    "        \n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev').get_text(strip=True) if product.find('div', class_='rev') else \"N/A\"\n",
    "      \n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'link': link,\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'rating': stars_rating,\n",
    "            'reviews_count': reviews\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Iterate over all URLs to scrape data and save to a CSV file\n",
    "for url in urls:\n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "        \n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Determine the output CSV file name based on the URL\n",
    "    if url == url1:\n",
    "        csv_filename = 'jumia_scraped_televisions.csv'  # Updated filename for televisions\n",
    "    else:\n",
    "        csv_filename = 'jumia_scraped_cookers.csv'\n",
    "\n",
    "    # Save the scraped product details to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[ \n",
    "            'link', 'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'rating', 'reviews_count'\n",
    "        ])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        for product in products_list:\n",
    "            writer.writerow(product)\n",
    "\n",
    "    print(f\"Scraped {len(products_list)} products from {url} and saved them to '{csv_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
