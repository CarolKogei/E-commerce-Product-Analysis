{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webscraping Products\n",
    "Tvs \n",
    "    * Smart\n",
    "    * Digital\n",
    "Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request url\n",
    "url1 = 'https://www.jumia.co.ke/smart-tvs-2282/'\n",
    "url2 = 'https://www.jumia.co.ke/digital-tvs/'\n",
    "url3 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'\n",
    "\n",
    "#list urls\n",
    "urls = [url1, url2, url3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check url status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending GET requests to each URL\n",
    "responses = [requests.get(url) for url in urls]\n",
    "\n",
    "# Output the status code of each response\n",
    "for response in responses:\n",
    "    print(response.url, response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "\n",
    "# Function to scrape all product details from the page\n",
    "def scrape_product_details(url):\n",
    "    # Send GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags with class 'core' that contain product information\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'href': link,\n",
    "            'item_name': name,\n",
    "            'prc': price,\n",
    "            'old': old_price,\n",
    "            'bdg_dsct_sm': discount,\n",
    "            'discount': discount,\n",
    "            'item_id': item_id,\n",
    "            'item_brand': item_brand,\n",
    "            'stars_s': stars_s,\n",
    "            'rev': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Scrape product details from the provided URL\n",
    "products = scrape_product_details(url)\n",
    "\n",
    "# Save the product details to a CSV file\n",
    "with open(\"jumia_scraped_product_details.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'href', 'item_name', 'prc', 'old', 'bdg_dsct_sm', 'discount', 'item_id', 'item_brand', 'stars_s', 'rev'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    # Write each product's details as a row\n",
    "    for product in products:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(products)} products and saved them to 'jumia_scraped_product_details.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the page to scrape\n",
    "base_url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "\n",
    "# Function to scrape all product details from a specific page\n",
    "def scrape_product_details_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <a> tags with class 'core' that contain product information\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'href': link,\n",
    "            'item_name': name,\n",
    "            'prc': price,\n",
    "            'old': old_price,\n",
    "            'bdg_dsct_sm': discount,\n",
    "            'discount': discount,\n",
    "            'item_id': item_id,\n",
    "            'item_brand': item_brand,\n",
    "            'stars_s': stars_s,\n",
    "            'rev': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the last page link\n",
    "    last_page_link = soup.find('a', class_='pg', aria_label='Last Page')\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        # Extract the page number from the href\n",
    "        last_page_url = last_page_link['href']\n",
    "        last_page_number = int(last_page_url.split('page=')[1].split('#')[0])\n",
    "        print(f\"Last page detected: {last_page_number}\")  # Print to confirm\n",
    "        return last_page_number\n",
    "    else:\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Scrape product details from all pages\n",
    "def scrape_all_products(base_url):\n",
    "    # Get the total number of pages\n",
    "    last_page_number = get_last_page_number(base_url)\n",
    "    all_products = []\n",
    "\n",
    "    # Iterate over each page and scrape the products\n",
    "    for page in range(1, last_page_number + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{base_url}?page={page}#catalog-listing\"\n",
    "        products_on_page = scrape_product_details_from_page(page_url)\n",
    "        all_products.extend(products_on_page)\n",
    "    \n",
    "    return all_products\n",
    "\n",
    "# Scrape all product details from all pages\n",
    "products = scrape_all_products(base_url)\n",
    "\n",
    "# Save the product details to a CSV file\n",
    "with open(\"jumia_scraped_all_products.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'href', 'item_name', 'prc', 'old', 'bdg_dsct_sm', 'discount', 'item_id', 'item_brand', 'stars_s', 'rev'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    # Write each product's details as a row\n",
    "    for product in products:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(products)} products and saved them to 'jumia_scraped_all_products.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# User-Agent headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_last_page_number(url):\n",
    "    # Make the request to get the page content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Ensure the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Return 1 if the request failed\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Debug: Let's print out all the <a> tags to check their contents\n",
    "    all_links = soup.find_all('a', class_='pg')  # Find all <a> tags with class 'pg'\n",
    "    print(\"All <a> tags with class 'pg':\")\n",
    "    for link in all_links:\n",
    "        print(link)\n",
    "\n",
    "    # Find the <a> tag with the aria-label 'Last Page' (check its presence)\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    # Debug: Print out the <a> tag for the last page link if found\n",
    "    if last_page_link:\n",
    "        print(f\"Last page link found: {last_page_link}\")\n",
    "    else:\n",
    "        print(\"No 'Last Page' link found.\")\n",
    "\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        # Extract the href value\n",
    "        last_page_url = last_page_link['href']\n",
    "        print(f\"Last page link URL: {last_page_url}\")  # Debug print statement\n",
    "        \n",
    "        # Extract the page number from the URL (after '?page=' and before '#')\n",
    "        try:\n",
    "            # Split the URL to extract the page number\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            last_page_number = int(page_number)  # Convert the extracted page number to integer\n",
    "            print(f\"Last page detected: {last_page_number}\")  # Confirm output\n",
    "            return last_page_number\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "\n",
    "# Testing the function with the base URL\n",
    "base_url = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "get_last_page_number(base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'link': link,\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'ratings': stars_s,\n",
    "            'reviews_count': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "# Get the last page number\n",
    "last_page = get_last_page_number(url3)\n",
    "\n",
    "# Initialize an empty list to store all products\n",
    "cookers = []\n",
    "\n",
    "# Iterate through all pages from 1 to the last page\n",
    "for page_num in range(1, last_page + 1):\n",
    "    page_url = f\"{url3}?page={page_num}#catalog-listing\"\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Scrape the products from the current page\n",
    "    products = scrape_product_details(page_url)\n",
    "    cookers.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "    # Sleep for a random time between requests to avoid overwhelming the server\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# Save the scraped product details to a CSV file\n",
    "with open(\"jumia_scraped_cookers1.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'link', 'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'ratings', 'reviews_count'\n",
    "    ])\n",
    "    writer.writeheader()  # Write the header row\n",
    "    \n",
    "    for product in cookers:\n",
    "        writer.writerow(product)\n",
    "\n",
    "print(f\"Scraped {len(cookers)} products and saved them to 'jumia_scraped_cookers1.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape SmartTvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests # make HTTP requests to fetch web pages content\n",
    "from bs4 import BeautifulSoup # parse HTML and XML docs for easier data extraction\n",
    "import csv # write scraped data into CSV file\n",
    "import time # introduce delays between requests to avoid server overload\n",
    "import random # vary time delays to simulate human-like behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWedKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_product_details(url):\n",
    "    reponse = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative url)\n",
    "        link = product['href'] if 'href' in product.attrs else None#\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of the Last Page\n",
    "def get_last_page_number(url):\n",
    "    response =requests.get(url, headers=headers)\n",
    "    if response.status_code !=200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1 # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link =soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1 # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1 # Default to 1 page if no last page is found\n",
    "\n",
    "last_page = get_last_page_number(url1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate for all page number of product listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# List of URLs to scrape\n",
    "url1 = 'https://www.jumia.co.ke/smart-tvs-2282/'\n",
    "url2 = 'https://www.jumia.co.ke/digital-tvs/'\n",
    "url3 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'\n",
    "\n",
    "urls = [url1, url2, url3]\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        # Extract product name from 'data-gtm-name' attribute\n",
    "        name = product.get('data-gtm-name', \"N/A\")  # Extract from the 'data-gtm-name' attribute\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars')\n",
    "        stars_s = stars_rating.get_text(strip=True) if stars_rating else \"N/A\"\n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        rev = reviews.get_text(strip=True) if reviews else \"N/A\"\n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'link': link,\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'ratings': stars_s,\n",
    "            'reviews_count': rev\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Iterate over all URLs to scrape data and save to a CSV file\n",
    "for url in urls:\n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "        \n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Determine the output CSV file name based on the URL\n",
    "    if url == url1:\n",
    "        csv_filename = 'jumia_scraped_smart-tvs.csv'\n",
    "    elif url == url2:\n",
    "        csv_filename = 'jumia_scraped_digital-tvs.csv'\n",
    "    else:\n",
    "        csv_filename = 'jumia_scraped_cookers.csv'\n",
    "\n",
    "    # Save the scraped product details to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\n",
    "            'link', 'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'ratings', 'reviews_count'\n",
    "        ])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        for product in products_list:\n",
    "            writer.writerow(product)\n",
    "\n",
    "    print(f\"Scraped {len(products_list)} products from {url} and saved them to '{csv_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "# User-Agent headers for requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# List of URLs to scrape (only one URL for televisions now)\n",
    "url1 = 'https://www.jumia.co.ke/televisions/#catalog-listing'  # Updated to televisions\n",
    "url3 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'  # Remains unchanged\n",
    "\n",
    "urls = [url1, url3]  # Removed the duplicate url2\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return 1  # Default to 1 if the request fails\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "        \n",
    "        # Extract product name (from the 'name' div)\n",
    "        # Extract product name from 'data-gtm-name' attribute\n",
    "        name = product.get('data-gtm-name', \"N/A\")  # Extract from the 'data-gtm-name' attribute\"\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract old price (if available) from the 'old' div\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        \n",
    "        # Extract discount (if available) from the 'bdg _dsct _sm' div\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars' class)\n",
    "        stars_rating = product.find('div', class_='stars').get_text(strip=True) if product.find('div', class_='stars') else \"N/A\"\n",
    "        \n",
    "        \n",
    "        # Extract reviews (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev').get_text(strip=True) if product.find('div', class_='rev') else \"N/A\"\n",
    "      \n",
    "        \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'link': link,\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'rating': stars_rating,\n",
    "            'reviews_count': reviews\n",
    "        })\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "# Iterate over all URLs to scrape data and save to a CSV file\n",
    "for url in urls:\n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "        \n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Determine the output CSV file name based on the URL\n",
    "    if url == url1:\n",
    "        csv_filename = 'jumia_scraped_televisions.csv'  # Updated filename for televisions\n",
    "    else:\n",
    "        csv_filename = 'jumia_scraped_cookers.csv'\n",
    "\n",
    "    # Save the scraped product details to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[ \n",
    "            'link', 'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'rating', 'reviews_count'\n",
    "        ])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        for product in products_list:\n",
    "            writer.writerow(product)\n",
    "\n",
    "    print(f\"Scraped {len(products_list)} products from {url} and saved them to '{csv_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Price: KSh 15,995\n",
      "Discount: 37%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to extract old price and discount from a webpage\n",
    "def extract_price_and_discount(url):\n",
    "    # Send HTTP request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    old_price = None\n",
    "    discount = None\n",
    "\n",
    "    # Try to extract data based on the first format\n",
    "    price_div_1 = soup.find('div', class_='-dif -i-ctr')\n",
    "    if price_div_1:\n",
    "        # Get the old price (which is the current price in this case)\n",
    "        current_price = price_div_1.find('span', class_='-tal -gy5 -lthr -fs16 -pvxs -ubpt')\n",
    "        discount_span = price_div_1.find('span', class_='bdg _dsct _dyn -mls')\n",
    "        \n",
    "        if current_price and discount_span:\n",
    "            current_price_text = current_price.get_text().strip()\n",
    "            discount_text = discount_span.get_text().strip()\n",
    "            \n",
    "            # Extract the old price directly (this is the discounted price)\n",
    "            old_price = current_price_text\n",
    "            discount = discount_text\n",
    "\n",
    "    # Try to extract data based on the second format\n",
    "    if not old_price:  # If not found in the first format, check the second format\n",
    "        price_div_2 = soup.find('div', class_='s-prc-w')\n",
    "        if price_div_2:\n",
    "            old_price_tag = price_div_2.find('div', class_='old')\n",
    "            discount_div = price_div_2.find('div', class_='bdg _dsct _sm')\n",
    "\n",
    "            if old_price_tag and discount_div:\n",
    "                old_price = old_price_tag.get_text().strip()\n",
    "                discount = discount_div.get_text().strip()\n",
    "\n",
    "    # Return the results\n",
    "    return old_price, discount\n",
    "\n",
    "# Example URL (replace with the actual URL of the webpage you want to scrape)\n",
    "url = 'https://www.jumia.co.ke/televisions/#catalog-listing'\n",
    "\n",
    "# Extract old price and discount\n",
    "old_price, discount = extract_price_and_discount(url)\n",
    "\n",
    "if old_price and discount:\n",
    "    print(f\"Old Price: {old_price}\")\n",
    "    print(f\"Discount: {discount}\")\n",
    "else:\n",
    "    print(\"Could not extract price and discount.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
