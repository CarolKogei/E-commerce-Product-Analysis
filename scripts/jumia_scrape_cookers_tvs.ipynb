{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TV scraping process...\n",
      "Scraped page 1, found 40 products\n",
      "Scraped page 2, found 40 products\n",
      "Scraped page 3, found 40 products\n",
      "Scraped page 4, found 40 products\n",
      "Scraped page 5, found 40 products\n",
      "Scraped page 6, found 40 products\n",
      "Scraped page 7, found 40 products\n",
      "Scraped page 8, found 40 products\n",
      "Scraped page 9, found 40 products\n",
      "Scraped page 10, found 40 products\n",
      "Scraped page 11, found 40 products\n",
      "Successfully scraped 440 TV products and saved to data/scraped/jumia_tvs.csv\n",
      "\n",
      "Starting cookers scraping process...\n",
      "Scraped cookers page 1, found 40 products\n",
      "Scraped cookers page 2, found 40 products\n",
      "Scraped cookers page 3, found 40 products\n",
      "Scraped cookers page 4, found 40 products\n",
      "Scraped cookers page 5, found 40 products\n",
      "Scraped cookers page 6, found 40 products\n",
      "Scraped cookers page 7, found 40 products\n",
      "Scraped cookers page 8, found 40 products\n",
      "Scraped cookers page 9, found 40 products\n",
      "Scraped cookers page 10, found 40 products\n",
      "Scraped cookers page 11, found 40 products\n",
      "Scraped cookers page 12, found 40 products\n",
      "Scraped cookers page 13, found 40 products\n",
      "Scraped cookers page 14, found 40 products\n",
      "Scraped cookers page 15, found 40 products\n",
      "Scraped cookers page 16, found 40 products\n",
      "Scraped cookers page 17, found 40 products\n",
      "Scraped cookers page 18, found 40 products\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "def clean_price(price_str):\n",
    "    if not price_str:\n",
    "        return None\n",
    "    return float(re.sub(r'[^\\d.]', '', price_str))\n",
    "\n",
    "def extract_product_info(product):\n",
    "    try:\n",
    "        # Find the core link element that contains all data attributes\n",
    "        core_link = product.find('a', class_='core')\n",
    "        if not core_link:\n",
    "            return None\n",
    "            \n",
    "        # Extract item_id, brand, and categories using data-ga4 attributes\n",
    "        item_id = core_link.get('data-ga4-item_id', \"N/A\")\n",
    "        item_brand = core_link.get('data-ga4-item_brand', \"N/A\")\n",
    "        item_name = core_link.get('data-ga4-item_name', \"N/A\")\n",
    "        \n",
    "        # Categories\n",
    "        category = core_link.get('data-ga4-item_category', \"Electronics\")\n",
    "        subcategory = core_link.get('data-ga4-item_category2', \"Television & Video\")\n",
    "        subcategory2 = core_link.get('data-ga4-item_category3', \"Televisions\")\n",
    "        subcategory3 = core_link.get('data-ga4-item_category4', \"\")\n",
    "        #subcategory4 = core_link.get('data-ga4-item_category5')\n",
    "        \n",
    "        # Extract product URL\n",
    "        product_url = core_link['href']\n",
    "        \n",
    "        # Price information\n",
    "        price_container = product.find('div', class_='prc')\n",
    "        current_price = clean_price(price_container.text.strip()) if price_container else None\n",
    "        \n",
    "        old_price_container = product.find('div', class_='old')\n",
    "        old_price = clean_price(old_price_container.text.strip()) if old_price_container else None\n",
    "        \n",
    "        # Discount with updated class\n",
    "        discount_container = product.find('div', class_='bdg _dsct _sm')\n",
    "        discount = discount_container.text.strip() if discount_container else None\n",
    "        if discount:\n",
    "            discount = int(discount.replace('%', '').replace('-', ''))\n",
    "        \n",
    "        # Extract ratings and reviews\n",
    "        rating = product.find('div', class_='rev')\n",
    "        if rating:\n",
    "            stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "            reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "        else:\n",
    "            stars = \"N/A\"\n",
    "            reviews_count = \"N/A\"\n",
    "        \n",
    "                \n",
    "        return {\n",
    "            'name': item_name,\n",
    "            'item_id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'price': current_price,\n",
    "            'old_price': old_price,\n",
    "            'discount': discount,\n",
    "            'stars_rating': stars,\n",
    "            'reviews_count': reviews_count,\n",
    "            'category': category,\n",
    "            'subcategory': subcategory,\n",
    "            'subcategory2': subcategory2,\n",
    "            'subcategory3': subcategory3,\n",
    "            'source': 'Jumia',\n",
    "            'url': f'https://www.jumia.co.ke{product_url}',\n",
    "            'scraping_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_jumia_tvs():\n",
    "    base_url = 'https://www.jumia.co.ke/televisions/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    all_products = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"{base_url}?page={page}#catalog-listing\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            products = soup.find_all('article', class_='prd _fb col c-prd')\n",
    "            \n",
    "            if not products:\n",
    "                break\n",
    "                \n",
    "            for product in products:\n",
    "                product_info = extract_product_info(product)\n",
    "                if product_info:\n",
    "                    all_products.append(product_info)\n",
    "            \n",
    "            print(f\"Scraped page {page}, found {len(products)} products\")\n",
    "            page += 1\n",
    "            \n",
    "            # Add delay to be respectful to the server\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return all_products\n",
    "\n",
    "def scrape_jumia_cookers():\n",
    "    base_url = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    all_products = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"{base_url}?page={page}#catalog-listing\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            products = soup.find_all('article', class_='prd _fb col c-prd')\n",
    "            \n",
    "            if not products:\n",
    "                break\n",
    "                \n",
    "            for product in products:\n",
    "                product_info = extract_product_info(product)\n",
    "                if product_info:\n",
    "                    # Update source and category for cookers\n",
    "                    product_info['source'] = 'Jumia Cookers'\n",
    "                    product_info['category'] = 'Home Appliances'\n",
    "                    product_info['subcategory'] = 'Cooking Appliances'\n",
    "                    all_products.append(product_info)\n",
    "            \n",
    "            print(f\"Scraped cookers page {page}, found {len(products)} products\")\n",
    "            page += 1\n",
    "            \n",
    "            # Add delay to be respectful to the server\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping cookers page {page}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return all_products\n",
    "\n",
    "def main():\n",
    "    # Create data directories if they don't exist\n",
    "    os.makedirs('data/scraped', exist_ok=True)\n",
    "    \n",
    "    # Scrape TVs\n",
    "    print(\"Starting TV scraping process...\")\n",
    "    tv_products = scrape_jumia_tvs()\n",
    "    \n",
    "    # Save TV products to CSV\n",
    "    if tv_products:\n",
    "        df_tvs = pd.DataFrame(tv_products)\n",
    "        tv_output_file = 'data/scraped/jumia_tvs.csv'\n",
    "        df_tvs.to_csv(tv_output_file, index=False)\n",
    "        print(f\"Successfully scraped {len(tv_products)} TV products and saved to {tv_output_file}\")\n",
    "    else:\n",
    "        print(\"No TV products were scraped\")\n",
    "    \n",
    "    # Scrape Cookers\n",
    "    print(\"\\nStarting cookers scraping process...\")\n",
    "    cooker_products = scrape_jumia_cookers()\n",
    "    \n",
    "    # Save Cooker products to CSV\n",
    "    if cooker_products:\n",
    "        df_cookers = pd.DataFrame(cooker_products)\n",
    "        cookers_output_file = 'data/scraped/jumia_cookers.csv'\n",
    "        df_cookers.to_csv(cookers_output_file, index=False)\n",
    "        print(f\"Successfully scraped {len(cooker_products)} cooker products and saved to {cookers_output_file}\")\n",
    "    else:\n",
    "        print(\"No cooker products were scraped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
