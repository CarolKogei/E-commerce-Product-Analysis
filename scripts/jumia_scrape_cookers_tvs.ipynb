{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests # make HTTP requests to fetch web pages content\n",
    "from bs4 import BeautifulSoup # parse HTML and XML docs for easier data extraction\n",
    "import csv # write scraped data into CSV file\n",
    "import time # introduce delays between requests to avoid server overload\n",
    "import re #regular expression\n",
    "import random # vary time delays to simulate human-like behaviour\n",
    "import psycopg2 #py package to interact with PostgreSQL\n",
    "from psycopg2 import sql\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('jumia_scraping.log'),\n",
    "        logging.StreamHandler()  # Also show logs in console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('jumia_scraping.log'),\n",
    "        logging.StreamHandler()  # Also show logs in console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Webscraping Page: Jumia**\n",
    "\n",
    "_Products to scrape:_  \n",
    "1. Tvs \n",
    "    * Smart\n",
    "    * Digital\n",
    "2. Cookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request url\n",
    "url1 = 'https://www.jumia.co.ke/televisions/#catalog-listing' # TVs url\n",
    "url2 = 'https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing' # Cookers url\n",
    "\n",
    "# User-Agent headers for automating requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check url status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.jumia.co.ke/televisions/#catalog-listing - status: 200\n",
      "url: https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing - status: 200\n",
      "All pages retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sending GET requests to each URL and check response\n",
    "def check_response(urls, headers):\n",
    "    # Output the status code of each response\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response =requests.get(url, headers=headers)\n",
    "            print(f\"url: {response.url} - status: { response.status_code}\")\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "                return 1  # Return 1 if the request fails\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle GET request exceptions (timeout, connection/http errors etc.)\n",
    "            print(f\"An error occured: {e}\")\n",
    "            return 1  # Return 1 if an error occurs during the request\n",
    "        \n",
    "    return 0  # Return 0 if no error\n",
    "\n",
    "# List of urls\n",
    "urls = [url1, url2]\n",
    "responses = check_response(urls, headers=headers) # Check all url GET requests responses\n",
    "\n",
    "# Check result\n",
    "if responses == 0:\n",
    "    print(\"All pages retrieved successfully.\")\n",
    "else:\n",
    "    print(\"Some pages failed to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that the webpages are paginated, there is need to navigatethrough, and retrieve data from each page.\n",
    "The last page in the page numbers is identified, then all pages are iterated.\n",
    "\n",
    "The pagination URL parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_product_details(url, headers):\n",
    "    products = []\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the catalog listing div\n",
    "        catalog_listing = soup.find('div', {'id': 'catalog-listing'})\n",
    "                \n",
    "        if catalog_listing:\n",
    "                \n",
    "            products = []\n",
    "            for product in catalog_listing:\n",
    "                try:\n",
    "                    # Find the core link which contains most product data\n",
    "                    core_link = product.find('a', {'class': 'core'})\n",
    "                    if not core_link:\n",
    "                        continue\n",
    "\n",
    "                    # Extract data from data-ga4 attributes which contain the most reliable info\n",
    "                    item_id = core_link.get('data-ga4-item_id', \"N/A\")\n",
    "                    name = core_link.get('data-ga4-item_name', \"N/A\")\n",
    "                    brand = core_link.get('data-ga4-item_brand', \"N/A\")\n",
    "                    price = core_link.get('data-ga4-price', \"N/A\")\n",
    "                    discount = core_link.get('data-ga4-discount', \"N/A\")\n",
    "                    category = core_link.get('data-ga4-item_category', \"N/A\")\n",
    "                    subcategory = core_link.get('data-ga4-item_category2', \"N/A\")\n",
    "                \n",
    "                    # Get product URL\n",
    "                    product_url = core_link.get('href')\n",
    "                    if product_url:\n",
    "                        product_url = f\"https://www.jumia.co.ke{product_url}\"\n",
    "                    else:\n",
    "                        product_url = \"N/A\"\n",
    "\n",
    "                    # Extract displayed price and old price\n",
    "                    price_elem = product.find('div', {'class': 'prc'})\n",
    "                    displayed_price = price_elem.text.strip() if price_elem else \"N/A\"\n",
    "                    \n",
    "                    old_price_elem = product.find('div', {'class': 'old'})\n",
    "                    old_price = old_price_elem.text.strip() if old_price_elem else \"N/A\"\n",
    "\n",
    "                    # Extract rating information\n",
    "                    stars_elem = product.find('div', {'class': 'stars _s'})\n",
    "                    reviews_count = \"0\"\n",
    "                    rating = \"N/A\"\n",
    "                    \n",
    "                    if stars_elem:\n",
    "                        rating_text = stars_elem.text.strip()\n",
    "                        if 'out of 5' in rating_text:\n",
    "                            rating = rating_text.split('out of 5')[0].strip()\n",
    "                        \n",
    "                        # Find review count\n",
    "                        rev_elem = product.find('div', {'class': 'rev'})\n",
    "                        if rev_elem:\n",
    "                            reviews_text = rev_elem.text\n",
    "                            reviews_count = reviews_text.strip('()') if '(' in reviews_text else \"0\"\n",
    "\n",
    "                    # Check if it's from official store\n",
    "                    is_official = bool(product.find('div', {'class': 'bdg _mall _xs'}))\n",
    "\n",
    "                    # Check if express shipping is available\n",
    "                    has_express = bool(product.find('svg', {'class': 'ic xprss'}))\n",
    "\n",
    "                    if item_id != \"N/A\":  # Only add products with valid IDs\n",
    "                        product_info = {\n",
    "                            \"name\": name,\n",
    "                            \"price\": displayed_price,\n",
    "                            \"price\": price,  # Numerical price value\n",
    "                            \"old_price\": old_price,\n",
    "                            \"discount\": discount,\n",
    "                            \"rating\": rating,\n",
    "                            \"reviews\": reviews_count,\n",
    "                            \"product_id\": item_id,\n",
    "                            \"brand\": brand,\n",
    "                            \"url\": product_url,\n",
    "                            #\"size\": product_size\n",
    "                            \"category\": category,\n",
    "                            \"subcategory\": subcategory,\n",
    "                            \"official_store\": is_official,\n",
    "                            \"express_shipping\": has_express\n",
    "                        }\n",
    "                    products.append(product_info)\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error extracting product details: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "        logging.info(f\"Successfully extracted {len(products)} valid products\")\n",
    "        \n",
    "        # Optional delay between requests\n",
    "        time.sleep(random.uniform(1, 3))  # Random delay between 1 and 3 seconds\n",
    "\n",
    "        return products\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request failed for {url}: {str(e)}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error scraping {url}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get the last page number\n",
    "def get_last_page_number(urls, headers):\n",
    "    \"\"\"\n",
    "    get the last page number in a catalog listing for pagination iteration\n",
    "\n",
    "    args:\n",
    "        urls: webpage URL or URLs\n",
    "        headers: User-Agent headers\n",
    "    \"\"\"\n",
    "    # Parse webpage content with BeautifulSoup\n",
    "    #for url in urls:\n",
    "    response =requests.get(urls, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find link for last page using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "\n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url =last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_postgresql(products, db_params, table_name):\n",
    "    #conn = None\n",
    "    #cursor = None\n",
    "    try:\n",
    "        # Connect to PostgreSQL database\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        logging.info(f\"Attempting to insert {len(products)} products into {table_name}\")\n",
    "        \n",
    "        # Create table if it doesn't exist\n",
    "        create_table_query = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                product_name TEXT,\n",
    "                product_id TEXT UNIQUE,\n",
    "                price TEXT,\n",
    "                old_price TEXT,\n",
    "                discount TEXT,\n",
    "                brand TEXT,\n",
    "                rating TEXT,\n",
    "                reviews TEXT,\n",
    "                category TEXT,\n",
    "                subcategory TEXT,\n",
    "                official_store BOOLEAN,\n",
    "                express_shipping BOOLEAN,              \n",
    "                url TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} created or verified\")\n",
    "        \n",
    "        print(f\"Connected successfully. Inserting {len(products)} products...\")\n",
    "        \n",
    "        # Insert data into the specified table\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} (product_name, price, old_price, discount, rating, item_id, item_brand, product_url)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        insert_data = []\n",
    "        for product in products:\n",
    "            # Format data to match schema exactly\n",
    "            row = (\n",
    "                product[\"name\"],\n",
    "                product[\"price\"],\n",
    "                product[\"old_price\"],\n",
    "                product[\"discount\"],\n",
    "                #f\"{product['rating']} ({product['reviews_count']} reviews)\",\n",
    "                product[\"rating\"],\n",
    "                product[\"reviews\"],\n",
    "                product[\"product_id\"],\n",
    "                product[\"brand\"],\n",
    "                #product[\"size\"],\n",
    "                #product[\"type\"],\n",
    "                #product[\"source\"]\n",
    "                product[\"category\"],\n",
    "                product[\"subcategory\"],\n",
    "                product[\"url\"]\n",
    "            )\n",
    "            insert_data.append(row)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database error while processing {table_name}: {str(e)}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "        \n",
    "               \n",
    "        # Verify final count\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        final_count = cursor.fetchone()[0]\n",
    "        print(f\"\\nFinal count in {table_name}: {final_count}\")\n",
    "        \n",
    "        # Show sample of inserted data\n",
    "        print(\"\\nVerifying inserted data...\")\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT 1\")\n",
    "        sample = cursor.fetchone()\n",
    "        if sample:\n",
    "            print(\"Sample record:\", sample)\n",
    "        else:\n",
    "            print(\"No records found in database!\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save(urls, headers, db_params, table_name):\n",
    "    print(f\"\\nStarting scrape for {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Get last page number\n",
    "        last_page = get_last_page_number(urls, headers)\n",
    "        print(f\"Found {last_page} pages to scrape\")\n",
    "        \n",
    "        # Initialize products list\n",
    "        products = []\n",
    "        \n",
    "        # Scrape each page\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            page_url = f\"{urls}?page={page_num}#catalog-listing\"\n",
    "            print(f\"Scraping page {page_num}/{last_page}\")\n",
    "            \n",
    "            # Scrape products from current page\n",
    "            page_products = scrape_product_details(page_url, headers)\n",
    "            print(f\"Found {len(page_products)} products on page {page_num}\")\n",
    "            products.extend(page_products)\n",
    "            \n",
    "            # Random delay\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        print(f\"Total products scraped: {len(products)}\")\n",
    "        \n",
    "        # Save products to database\n",
    "        if products:\n",
    "            print(f\"\\nFirst product data (sample):\")\n",
    "            first_product = products[0]\n",
    "            for key, value in first_product.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "                \n",
    "            print(f\"\\nAttempting to save {len(products)} products to {table_name}\")\n",
    "            save_to_postgresql(products, db_params, table_name)\n",
    "        else:\n",
    "            print(\"No products found to save!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in scrape_and_save: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "        # URLs\n",
    "        url1 = \"https://www.jumia.co.ke/televisions/\"\n",
    "        url2 = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "        \n",
    "        # Database connection parameters\n",
    "        db_params = {\n",
    "            \"host\": \"localhost\",     # Database host\n",
    "            \"database\": \"e-analytics_db\",   # Database name\n",
    "            \"user\": \"postgres\",     # Database user\n",
    "            \"password\": \"password\"  # Database password\n",
    "        }\n",
    "        # Scrape and save data for both URLs\n",
    "        scrape_and_save(url1, headers, db_params, \"jumia_televisions\")\n",
    "        scrape_and_save(url2, headers, db_params, \"jumia_cookers\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Main execution error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://www.jumia.co.ke/televisions/#catalog-listing\n",
      "Scraping page 1 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 2 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 3 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 4 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 5 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 6 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 7 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 8 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 9 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 10 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 11 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 12 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 13 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 14 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 15 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 16 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 17 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 18 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 19 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 20 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 21 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 22 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 23 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 24 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 25 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 26 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 27 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 28 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 29 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 30 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 31 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 32 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 33 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 34 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 35 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 36 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 37 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 38 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 39 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 40 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 41 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 42 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 43 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 44 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 45 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 46 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 47 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 48 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 49 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Scraping page 50 from https://www.jumia.co.ke/televisions/#catalog-listing...\n",
      "Saved 50 products from https://www.jumia.co.ke/televisions/#catalog-listing to jumia_televisions table.\n",
      "Scraping URL: https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing\n",
      "Scraping page 1 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 2 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 3 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 4 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 5 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 6 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 7 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 8 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 9 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 10 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 11 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 12 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 13 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 14 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 15 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 16 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 17 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 18 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 19 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 20 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 21 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 22 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 23 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 24 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 25 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 26 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 27 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 28 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 29 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n",
      "Scraping page 30 from https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing...\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch and scrape product details from a single page\n",
    "def scrape_product_details(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    catalog_divs = soup.find_all('div', attrs={'data-catalog': 'true'})\n",
    "\n",
    "    products = []\n",
    "    for catalog_div in catalog_divs:\n",
    "        product = catalog_div.find('a', class_='core')\n",
    "        \n",
    "        if product:\n",
    "            name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "            price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "            old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "            discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "            \n",
    "            rating = product.find('div', class_='rev')\n",
    "            if rating:\n",
    "                stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "                reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "            else:\n",
    "                stars = \"N/A\"\n",
    "                reviews_count = \"N/A\"\n",
    "\n",
    "            item_id = product.get('data-gtm-id', \"N/A\")\n",
    "            item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "            product_url = product['href'] if product.has_attr('href') else \"N/A\"\n",
    "\n",
    "            # Store product info in a dictionary\n",
    "            products.append({\n",
    "                \"product_name\": name,\n",
    "                \"price\": price,\n",
    "                \"old_price\": old_price,\n",
    "                \"discount\": discount,\n",
    "                \"rating\": f\"{stars} ({reviews_count} reviews)\",\n",
    "                \"item_id\": item_id,\n",
    "                \"item_brand\": item_brand,\n",
    "                \"product_url\": f\"https://www.jumia.co.ke{product_url}\"\n",
    "            })\n",
    "        \n",
    "        # Optional delay between requests\n",
    "        time.sleep(random.uniform(1, 3))  # Random delay between 1 and 3 seconds\n",
    "\n",
    "    return products\n",
    "\n",
    "# Function to get the last page number\n",
    "def get_last_page_number(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the last page link\n",
    "        last_page_link = soup.find('a', {'aria-label': 'Last Page'})\n",
    "        \n",
    "        if last_page_link:\n",
    "            last_page_url = last_page_link.get('href', '')\n",
    "            \n",
    "            # Extract the page number from the URL\n",
    "            page_number_str = last_page_url.split('=')[-1]\n",
    "            page_number_str = page_number_str.split('#')[0]  # Remove any fragment identifier (#catalog-listing)\n",
    "            \n",
    "            page_number = int(page_number_str) if page_number_str.isdigit() else 1\n",
    "            return page_number\n",
    "        else:\n",
    "            logging.warning(f\"Last page link not found for {url}. Defaulting to page 1.\")\n",
    "            return 1\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching last page number for {url}: {str(e)}\")\n",
    "        return 1\n",
    "\n",
    "# Function to save data to PostgreSQL\n",
    "def save_to_postgresql(products, db_params, table_name):\n",
    "    try:\n",
    "        # Connect to your PostgreSQL database\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the specified table\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} (product_name, price, old_price, discount, rating, item_id, item_brand, product_url)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        for product in products:\n",
    "            cursor.execute(insert_query, (\n",
    "                product[\"product_name\"],\n",
    "                product[\"price\"],\n",
    "                product[\"old_price\"],\n",
    "                product[\"discount\"],\n",
    "                product[\"rating\"],\n",
    "                product[\"item_id\"],\n",
    "                product[\"item_brand\"],\n",
    "                product[\"product_url\"]\n",
    "            ))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Successfully inserted {len(products)} products into the {table_name} table.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Function to scrape and save data for a specific URL and table\n",
    "def scrape_and_save(url, headers, db_params, table_name):\n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    \n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url, headers)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "\n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url, headers)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Save the products to PostgreSQL after scraping all pages for this URL\n",
    "    if products_list:\n",
    "        #save_to_postgresql(products_list, db_params, table_name)\n",
    "        print(f\"Saved {len(products_list)} products from {url} to {table_name} table.\")\n",
    "    else:\n",
    "        print(f\"No products found on {url}.\")\n",
    "\n",
    "# Define your headers and the URLs to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Define url1 and url2\n",
    "url1 = \"https://www.jumia.co.ke/televisions/#catalog-listing\"\n",
    "#url2 = \"https://www.jumia.co.ke/cookers/#catalog-listing\" \n",
    "url2 = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing\"\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"localhost\",     # Database host\n",
    "    \"database\": \"e-analytics_db\",   # Database name\n",
    "    \"user\": \"postgres\",     # Database user\n",
    "    \"password\": \"password\"  # Database password\n",
    "}\n",
    "\n",
    "# Scrape and save data for both URLs\n",
    "scrape_and_save(url1, headers, db_params, \"jumia_televisions\")\n",
    "scrape_and_save(url2, headers, db_params, \"jumia_cookers\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
