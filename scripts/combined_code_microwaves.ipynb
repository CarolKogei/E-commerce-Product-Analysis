{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476\n",
      "Processing page: 1\n",
      "Processing page: 2\n",
      "Processing page: 3\n",
      "Processing page: 4\n",
      "Processing page: 5\n",
      "Processing page: 6\n",
      "Processing page: 7\n",
      "Processing page: 8\n",
      "Processing page: 9\n",
      "Processing page: 10\n",
      "Processing page: 11\n",
      "Processing page: 12\n",
      "Processing page: 13\n",
      "Processing page: 14\n",
      "Processing page: 15\n",
      "Processing page: 16\n",
      "Processing page: 17\n",
      "Processing page: 18\n",
      "Processing page: 19\n",
      "Processing page: 20\n",
      "Processing page: 21\n",
      "Processing page: 22\n",
      "Processing page: 23\n",
      "Processing page: 24\n",
      "Processing page: 25\n",
      "Processing page: 26\n",
      "Processing page: 27\n",
      "Processing page: 28\n",
      "Processing page: 29\n",
      "Processing page: 30\n",
      "Processing page: 31\n",
      "Processing page: 32\n",
      "Processing page: 33\n",
      "Processing page: 34\n",
      "Processing page: 35\n",
      "Processing page: 36\n",
      "Processing page: 37\n",
      "Processing page: 38\n",
      "Processing page: 39\n",
      "Processing page: 40\n",
      "Processing page: 41\n",
      "Processing page: 42\n",
      "Processing page: 43\n",
      "Processing page: 44\n",
      "Processing page: 45\n",
      "Processing page: 46\n",
      "Processing page: 47\n",
      "Processing page: 48\n",
      "Processing page: 49\n",
      "Processing page: 50\n",
      "Processing page: 51\n",
      "Processing page: 52\n",
      "Processing page: 53\n",
      "Processing page: 54\n",
      "Processing page: 55\n",
      "Processing page: 56\n",
      "Processing page: 57\n",
      "Processing page: 58\n",
      "Processing page: 59\n",
      "Processing page: 60\n",
      "Processing page: 61\n",
      "Processing page: 62\n",
      "Processing page: 63\n",
      "Processing page: 64\n",
      "Processing page: 65\n",
      "Processing page: 66\n",
      "Processing page: 67\n",
      "Processing page: 68\n",
      "Processing page: 69\n",
      "Processing page: 70\n",
      "Processing page: 71\n",
      "Processing page: 72\n",
      "Processing page: 73\n",
      "Processing page: 74\n",
      "Processing page: 75\n",
      "Processing page: 76\n",
      "Processing page: 77\n",
      "Processing page: 78\n",
      "Processing page: 79\n",
      "Processing page: 80\n",
      "Processing page: 81\n",
      "Processing page: 82\n",
      "Processing page: 83\n",
      "Processing page: 84\n",
      "Processing page: 85\n",
      "Processing page: 86\n",
      "Processing page: 87\n",
      "Processing page: 88\n",
      "Processing page: 89\n",
      "Processing page: 90\n",
      "Processing page: 91\n",
      "Processing page: 92\n",
      "Processing page: 93\n",
      "Processing page: 94\n",
      "Processing page: 95\n",
      "Processing page: 96\n",
      "Processing page: 97\n",
      "Processing page: 98\n",
      "Processing page: 99\n",
      "Processing page: 100\n",
      "Processing page: 101\n",
      "Processing page: 102\n",
      "Processing page: 103\n",
      "Processing page: 104\n",
      "Processing page: 105\n",
      "Processing page: 106\n",
      "Processing page: 107\n",
      "Processing page: 108\n",
      "Processing page: 109\n",
      "Processing page: 110\n",
      "Processing page: 111\n",
      "Processing page: 112\n",
      "Processing page: 113\n",
      "Processing page: 114\n",
      "Processing page: 115\n",
      "Processing page: 116\n",
      "Processing page: 117\n",
      "Processing page: 118\n",
      "Processing page: 119\n",
      "Processing page: 120\n",
      "Processing page: 121\n",
      "Processing page: 122\n",
      "Processing page: 123\n",
      "Processing page: 124\n",
      "Processing page: 125\n",
      "Processing page: 126\n",
      "Processing page: 127\n",
      "Processing page: 128\n",
      "Processing page: 129\n",
      "Processing page: 130\n",
      "Processing page: 131\n",
      "Processing page: 132\n",
      "Processing page: 133\n",
      "Processing page: 134\n",
      "Processing page: 135\n",
      "Processing page: 136\n",
      "Processing page: 137\n",
      "Processing page: 138\n",
      "Processing page: 139\n",
      "Processing page: 140\n",
      "Processing page: 141\n",
      "Processing page: 142\n",
      "Processing page: 143\n",
      "Processing page: 144\n",
      "Processing page: 145\n",
      "Processing page: 146\n",
      "Processing page: 147\n",
      "Processing page: 148\n",
      "Processing page: 149\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "baseurl = \"https://www.kilimall.co.ke/\"\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.6312.123 Safari/537.36 AOLShield/123.0.6312.3'}\n",
    "microwaves_links = []\n",
    "\n",
    "for x in range(1,42):\n",
    "    r = requests.get(f\"https://www.kilimall.co.ke/search?q=microwave&page={x}&source=search|enterSearch|microwave\")\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    microwaves_list = soup.find_all(\"div\", class_ = \"product-item\")\n",
    "\n",
    "    for microwave in microwaves_list:\n",
    "        for link in microwave.find_all(\"a\", href = True):\n",
    "            microwaves_links.append(\"https://www.kilimall.co.ke\" + link[\"href\"])\n",
    "\n",
    "print(len(microwaves_links)) \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open(\"kilimall_microwaves_scraped.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"microwaves_name\", \"microwaves_reviews\", \"microwaves_price\", \"microwaves_links\"])\n",
    "    writer.writeheader()  # Write column headers\n",
    "    \n",
    "    for x in range(1, 150):  # Adjust the range as needed\n",
    "        print(f\"Processing page: {x}\")\n",
    "        r = requests.get(f\"https://www.kilimall.co.ke/search?q=microwaves&page={x}&source=search|enterSearch|microwaves\")\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        items = soup.find_all(\"div\", class_=\"info-box\")\n",
    "        \n",
    "        for item in items:\n",
    "            try:\n",
    "                microwaves_name = item.find(\"p\", class_=\"product-title\").text.strip()\n",
    "                microwaves_price = item.find(\"div\", class_=\"product-price\").text.strip()\n",
    "                microwaves_reviews = item.find(\"span\", class_=\"reviews\").text.strip() if item.find(\"span\", class_=\"reviews\") else \"No reviews\"\n",
    "                \n",
    "                # Safely get the link\n",
    "                link_tag = item.find(\"a\", href=True)\n",
    "                microwaves_links = \"https://www.kilimall.co.ke\" + link_tag[\"href\"] if link_tag else \"No link available\"\n",
    "                \n",
    "                # Define a dictionary for each microwave\n",
    "                microwave_dict = {\n",
    "                    \"microwaves_name\": microwaves_name,\n",
    "                    \"microwaves_reviews\": microwaves_reviews,\n",
    "                    \"microwaves_price\": microwaves_price,\n",
    "                    \"microwaves_links\": microwaves_links,\n",
    "                }\n",
    "                \n",
    "                # Write the data directly to the CSV file\n",
    "                writer.writerow(microwave_dict)\n",
    "            except AttributeError:\n",
    "                # Skip items with missing data\n",
    "                continue\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Vivian.Obino1\\Desktop\\e-commerce analysis\\data\\scraped\\kilimall_microwaves_scraped.csv\")\n",
    "df_cleaned = df.dropna()\n",
    "df['microwaves_price']= df['microwaves_price'].str.strip('KSh,')\n",
    "df['microwaves_reviews'] = df['microwaves_reviews'].str.extract(r'(\\d+)').astype(int)\n",
    "# List of unwanted words (make sure all variations are included)\n",
    "unwanted_words = ['clearance', 'CLEARANCE', 'SALE', 'OFFER', 'Best', 'CHOOSE', 'Offers', 'QUALITY', 'THE', 'FUTURE',\n",
    "                  'EMBRACE', 'LATEST', 'TREND', 'MEGASALE', 'Buy', 'NOW', 'AND', 'ENJOY', 'UPTO', 'NEW', 'IMPROVED', \n",
    "                  'STAY', 'LOCKED', 'WITH', 'STOCK', 'kilimall', 'special', 'MAKE', 'YOUR', 'HOUSE', 'FEEL', 'LIKE', \n",
    "                  'Super', 'deal', 'quality', 'RESTOCKED', 'Share', 'this', 'product', 'Best', 'ARRIVALS', 'HURRY', \n",
    "                  'AND', 'PICK', 'YOURS', 'LIMITED', 'AN', 'NO', 'OTHER', 'PRICE', 'REDUCED', 'NOWBLACK', 'Angry', \n",
    "                  'mama', 'kitchen','EXPERIENCE', 'New', 'Arrival', 'Classy', 'sale', 'offer', 'best', 'discount', \n",
    "                  'cheap', 'deal', 'SALE','Promotions','OFFER','offer','TRUSTED','SOURCE','UPGRADE','THESE','TOP','DURABLE','LISTING','ON','Cooking','End','Original','OF','ALL','affordable']\n",
    "\n",
    "# Convert the unwanted_words list to lowercase for case-insensitive comparison\n",
    "unwanted_words = set(word.lower() for word in unwanted_words)\n",
    "\n",
    "# Function to remove unwanted words from a text\n",
    "def remove_unwanted_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  # Return as is if not a string\n",
    "    words = text.split()  # Split the text into individual words\n",
    "    cleaned_words = [word for word in words if word.lower() not in unwanted_words]  # Filter out unwanted words\n",
    "    return ' '.join(cleaned_words)  # Join the cleaned words back into a string\n",
    "\n",
    "file_path =(r\"C:\\Users\\Vivian.Obino1\\Desktop\\e-commerce analysis\\data\\scraped\\kilimall_microwaves_scraped.csv\") # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply the function to the relevant column (e.g., 'microwaves_name')\n",
    "df['microwaves_name'] = df['microwaves_name'].apply(remove_unwanted_words)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file_path = 'kilimall_clean_microwaves.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'kilimall_clean_microwaves.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove punctuation, emojis, and parentheses\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s*\\([^)]*\\)\\s*', '', text)  # Remove parentheses and their content\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove emojis and non-ASCII characters\n",
    "    return text.strip()\n",
    "  \n",
    "# Extract brand name and clean it\n",
    "def extract_brand_name(text):\n",
    "    words = clean_text(text).split()[:5]  # Get first five words\n",
    "    return ' '.join(words)\n",
    "def extract_capacity(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    match = re.search(r'\\b(\\d+\\.?\\d*)\\s*(L|Ltrs|Liters|Litres)\\b', text, re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n",
    "def remove_parentheses(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return re.sub(r'\\s*\\([^)]*\\)\\s*', '', text)\n",
    "\n",
    "# Apply the function to the 'microwave_reviews' column\n",
    "\n",
    "\n",
    "# Apply the brand name extraction\n",
    "df['microwaves_name'] = df['microwaves_name'].apply(extract_brand_name)  # Replace 'ColumnName' with the relevant column name\n",
    "df['Capacity'] = df['microwaves_name'].apply(extract_capacity) \n",
    "# Lines to remove\n",
    "lines_to_remove = [*range(12, 19), 83, 84, 127, 128, 131, *range(155, 160), *range(608, 624), \n",
    "                   *range(652, 658), *range(675, 683), *range(686, 692), 461, 463, *range(187, 197), \n",
    "                   *range(257, 267), 275, 342, 343, *range(769, 806), *range(832, 839)]\n",
    "\n",
    "# Drop specified rows (subtract 1 to account for zero-based index in Python)\n",
    "df.drop(index=[i-1 for i in lines_to_remove], inplace=True, errors='ignore')\n",
    "# Rename the columns\n",
    "df.columns = ['brand', 'reviews', 'price', 'microwaves_url','capacity']   \n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file_path = 'kilimall_clean_microwaves.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "df['price']= df['price'].str.strip('KSh,')\n",
    "def retain_text_in_parentheses(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  # Return as is if not a string\n",
    "    return re.sub(r'[()]', '', text)  # Remove only parentheses\n",
    "\n",
    "# Apply the function to the desired column\n",
    "df['reviews'] = df['reviews'].apply(retain_text_in_parentheses)\n",
    "df['price'] = df['price'].replace({',': ''}, regex=True)\n",
    "df.to_csv('kilimall_clean_microwaves.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
