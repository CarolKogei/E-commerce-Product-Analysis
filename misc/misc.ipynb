{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d --name my-container -e POSTGRES_PASSWORD=password -e POSTGRES_USER=postgres -e POSTGRES_DB=e-analytics_db -p 5432:5432 postgres:latest\n",
    "\n",
    "docker start my-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbconvert --to script example_notebook.ipynb --output ./scripts/example_script.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate over all URLs to scrape data and save to a CSV file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[43murls\u001b[49m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Get the last page number for the current URL\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     last_page \u001b[38;5;241m=\u001b[39m get_last_page_number(url, headers)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize an empty list to store all products\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate over all URLs to scrape data and save to a CSV file\n",
    "for url in urls:\n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url, headers)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "        \n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url, headers)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Determine the output CSV file name based on the URL\n",
    "    if url == url1:\n",
    "        csv_filename = 'data/scrapped/jumia_scraped_televisions.csv' \n",
    "    else:\n",
    "        csv_filename = 'data/scrapped/jumia_scraped_cookers.csv'\n",
    "\n",
    "    # Save the scraped product details to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[ \n",
    "            'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'rating', 'reviews_count', 'link'\n",
    "        ])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        for product in products_list:\n",
    "            writer.writerow(product)\n",
    "\n",
    "    print(f\"Scraped {len(products_list)} products from {url} and saved them to '{csv_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url, headers):\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "    \n",
    "        # Extract product name from 'data-gtm-name' attribute\n",
    "        name = product.get('data-gtm-name', \"N/A\")\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        old_price = product.find('div', class_='prc')['data-oprc']\n",
    "        discount = product.find('div', class_='bdg _dsct').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars _m _al' or 'stars _s' class)\n",
    "        stars_rating = product.find('div', class_='stars _m _al') or product.find('div', class_='stars _s')\n",
    "        if stars_rating:\n",
    "            rating = stars_rating.get_text(strip=True).split(\" out of \")[0]  # Extract the rating value (e.g., \"3.9\")\n",
    "        else:\n",
    "            rating = \"N/A\"\n",
    "        \n",
    "        # Extract reviews count (from the 'rev' class or 'verified ratings' link)\n",
    "        #reviews = product.find('div', class_='rev')\n",
    "        #if reviews:\n",
    "        #   reviews_count = reviews.get_text(strip=True).split('(')[-1].split(')')[0]  # Extract the review count (e.g., \"798\")\n",
    "        #else:\n",
    "        #   reviews_link = product.find('a', class_='-plxs _more')\n",
    "        #   reviews_count = reviews_link.get_text(strip=True).split('(')[-1].split(')')[0] if reviews_link else \"N/A\"\n",
    "\n",
    "        # Extract reviews count (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        if reviews:\n",
    "        # Use regular expression to find the number inside parentheses\n",
    "            reviews_count = re.search(r'\\((\\d+)\\)', reviews.get_text(strip=True))\n",
    "            reviews_count = reviews_count.group(1) if reviews_count else \"N/A\"\n",
    "        else:\n",
    "            reviews_link = product.find('a', class_='-plxs _more')\n",
    "            reviews_count = re.search(r'\\((\\d+)\\)', reviews_link.get_text(strip=True))\n",
    "            reviews_count = reviews_count.group(1) if reviews_count else \"N/A\"\n",
    "\n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'rating': rating,\n",
    "            'reviews_count': reviews,\n",
    "            'link': link,\n",
    "        })\n",
    "    \n",
    "    return product_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Function to fetch and scrape product details from a single page\n",
    "def scrape_product_details(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    catalog_divs = soup.find_all('div', attrs={'data-catalog': 'true'})\n",
    "\n",
    "    products = []\n",
    "    for catalog_div in catalog_divs:\n",
    "        product = catalog_div.find('a', class_='core')\n",
    "        \n",
    "        if product:\n",
    "            name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "            price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "            old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "            discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "            \n",
    "            rating = product.find('div', class_='rev')\n",
    "            if rating:\n",
    "                stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "                reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "            else:\n",
    "                stars = \"N/A\"\n",
    "                reviews_count = \"N/A\"\n",
    "\n",
    "            item_id = product.get('data-gtm-id', \"N/A\")\n",
    "            item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "            product_url = product['href'] if product.has_attr('href') else \"N/A\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to extract old price and discount from a webpage\n",
    "def extract_price_and_discount(url):\n",
    "    # Send HTTP request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    old_price = None\n",
    "    discount = None\n",
    "\n",
    "    # Try to extract data based on the first format\n",
    "    price_div_1 = soup.find('div', class_='-dif -i-ctr')\n",
    "    if price_div_1:\n",
    "        # Get the old price (which is the current price in this case)\n",
    "        current_price = price_div_1.find('span', class_='-tal -gy5 -lthr -fs16 -pvxs -ubpt')\n",
    "        discount_span = price_div_1.find('span', class_='bdg _dsct _dyn -mls')\n",
    "        \n",
    "        if current_price and discount_span:\n",
    "            current_price_text = current_price.get_text().strip()\n",
    "            discount_text = discount_span.get_text().strip()\n",
    "            \n",
    "            # Extract the old price directly (this is the discounted price)\n",
    "            old_price = current_price_text\n",
    "            discount = discount_text\n",
    "\n",
    "    # Try to extract data based on the second format\n",
    "    if not old_price:  # If not found in the first format, check the second format\n",
    "        price_div_2 = soup.find('div', class_='s-prc-w')\n",
    "        if price_div_2:\n",
    "            old_price_tag = price_div_2.find('div', class_='old')\n",
    "            discount_div = price_div_2.find('div', class_='bdg _dsct _sm')\n",
    "\n",
    "            if old_price_tag and discount_div:\n",
    "                old_price = old_price_tag.get_text().strip()\n",
    "                discount = discount_div.get_text().strip()\n",
    "\n",
    "    # Return the results\n",
    "    return old_price, discount\n",
    "\n",
    "# Example URL (replace with the actual URL of the webpage you want to scrape)\n",
    "url = 'https://www.jumia.co.ke/televisions/#catalog-listing'\n",
    "\n",
    "# Extract old price and discount\n",
    "old_price, discount = extract_price_and_discount(url)\n",
    "\n",
    "if old_price and discount:\n",
    "    print(f\"Old Price: {old_price}\")\n",
    "    print(f\"Discount: {discount}\")\n",
    "else:\n",
    "    print(\"Could not extract price and discount.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del from el.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url, headers):\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "    \n",
    "        name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "        discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "        \n",
    "        rating = product.find('div', class_='rev')\n",
    "        if rating:\n",
    "            stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "            reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "        else:\n",
    "            stars = \"N/A\"\n",
    "            reviews_count = \"N/A\"\n",
    "        \n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        product_url = product['href'] if product.has_attr('href') else \"N/A\"\n",
    "\n",
    "                \n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'rating': rating,\n",
    "            'reviews_count': reviews,\n",
    "            'link': link,\n",
    "        })\n",
    "\n",
    "\n",
    "        # Optional delay between requests\n",
    "        time.sleep(random.uniform(1, 3))  # Random delay between 1 and 3 seconds\n",
    "        \n",
    "    return product_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import psycopg2\n",
    "\n",
    "# Function to fetch and scrape product details from a single page\n",
    "def scrape_product_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    catalog_divs = soup.find_all('div', attrs={'data-catalog': 'true'})\n",
    "\n",
    "    products = []\n",
    "    for catalog_div in catalog_divs:\n",
    "        product = catalog_div.find('a', class_='core')\n",
    "        \n",
    "        if product:\n",
    "            name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "            price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "            old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "            discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "            \n",
    "            rating = product.find('div', class_='rev')\n",
    "            if rating:\n",
    "                stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "                reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "            else:\n",
    "                stars = \"N/A\"\n",
    "                reviews_count = \"N/A\"\n",
    "\n",
    "            item_id = product.get('data-gtm-id', \"N/A\")\n",
    "            item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "            product_url = product['href'] if product.has_attr('href') else \"N/A\"\n",
    "\n",
    "            # Store product info in a dictionary\n",
    "            products.append({\n",
    "                \"product_name\": name,\n",
    "                \"price\": price,\n",
    "                \"old_price\": old_price,\n",
    "                \"discount\": discount,\n",
    "                \"rating\": f\"{stars} ({reviews_count} reviews)\",\n",
    "                \"item_id\": item_id,\n",
    "                \"item_brand\": item_brand,\n",
    "                \"product_url\": f\"https://www.jumia.co.ke{product_url}\"\n",
    "            })\n",
    "        \n",
    "        # Optional delay between requests\n",
    "        time.sleep(random.uniform(1, 3))  # Random delay between 1 and 3 seconds\n",
    "\n",
    "    return products\n",
    "\n",
    "# Function to get the last page number from the website\n",
    "def get_last_page_number(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return 1  # Return 1 if failed to fetch page number\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Try to find the pagination section\n",
    "    pagination = soup.find('ul', class_='pagination')\n",
    "\n",
    "    # If no pagination is found, return 1 (assuming only one page)\n",
    "    if pagination is None:\n",
    "        print(\"No pagination found, assuming only one page.\")\n",
    "        return 1\n",
    "\n",
    "    # If pagination is found, find the last page number\n",
    "    try:\n",
    "        last_page = pagination.find_all('li')[-1].get_text(strip=True)\n",
    "        return int(last_page) if last_page.isdigit() else 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing pagination: {e}\")\n",
    "        return 1\n",
    "\n",
    "# Function to save data to PostgreSQL\n",
    "def save_to_postgresql(products, db_params):\n",
    "    try:\n",
    "        # Connect to your PostgreSQL database\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the database\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO jumia_televisions (product_name, price, old_price, discount, rating, item_id, item_brand, product_url)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        for product in products:\n",
    "            cursor.execute(insert_query, (\n",
    "                product[\"product_name\"],\n",
    "                product[\"price\"],\n",
    "                product[\"old_price\"],\n",
    "                product[\"discount\"],\n",
    "                product[\"rating\"],\n",
    "                product[\"item_id\"],\n",
    "                product[\"item_brand\"],\n",
    "                product[\"product_url\"]\n",
    "            ))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Successfully inserted {len(products)} products into the database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"localhost\",     # Database host\n",
    "    \"database\": \"e-analytics_db\",   # Database name\n",
    "    \"user\": \"postgres\",     # Database user\n",
    "    \"password\": \"password\"  # Database password\n",
    "}\n",
    "\n",
    "# Function to scrape and save data for multiple URLs\n",
    "def scrape_and_save(urls, db_params):\n",
    "    for url in urls:\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        \n",
    "        # Get the last page number for the current URL\n",
    "        last_page = get_last_page_number(url)\n",
    "\n",
    "        # Initialize an empty list to store all products\n",
    "        products_list = []\n",
    "\n",
    "        # Iterate through all pages from 1 to the last page\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "            print(f\"Scraping page {page_num} from {url}...\")\n",
    "\n",
    "            # Scrape the products from the current page\n",
    "            products = scrape_product_details(page_url)\n",
    "            products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "            # Sleep for a random time between requests to avoid overwhelming the server\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # Save the products to PostgreSQL after scraping all pages for this URL\n",
    "        if products_list:\n",
    "            save_to_postgresql(products_list, db_params)\n",
    "            print(f\"Saved {len(products_list)} products from {url} to PostgreSQL.\")\n",
    "        else:\n",
    "            print(f\"No products found on {url}.\")\n",
    "\n",
    "# Define your headers and the URLs to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# List of URLs to scrape (example)\n",
    "urls = [\n",
    "    \"https://www.jumia.co.ke/cookers/#catalog-listing\",\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Start the main scraping process\n",
    "scrape_and_save(urls, db_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and scrape product details from a single page\n",
    "def scrape_product_details(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    catalog_divs = soup.find_all('div', attrs={'data-catalog': 'true'})\n",
    "\n",
    "    products = []\n",
    "    for catalog_div in catalog_divs:\n",
    "        product = catalog_div.find('a', class_='core')\n",
    "        \n",
    "        if product:\n",
    "            name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "            price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "            old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "            discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "            \n",
    "            rating = product.find('div', class_='rev')\n",
    "            if rating:\n",
    "                stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "                reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "            else:\n",
    "                stars = \"N/A\"\n",
    "                reviews_count = \"N/A\"\n",
    "\n",
    "            item_id = product.get('data-gtm-id', \"N/A\")\n",
    "            item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "            product_url = product['href'] if product.has_attr('href') else \"N/A\"\n",
    "\n",
    "            # Store product info in a dictionary\n",
    "            products.append({\n",
    "                \"product_name\": name,\n",
    "                \"price\": price,\n",
    "                \"old_price\": old_price,\n",
    "                \"discount\": discount,\n",
    "                \"rating\": f\"{stars} ({reviews_count} reviews)\",\n",
    "                \"item_id\": item_id,\n",
    "                \"item_brand\": item_brand,\n",
    "                \"product_url\": f\"https://www.jumia.co.ke{product_url}\"\n",
    "            })\n",
    "        \n",
    "        # Optional delay between requests\n",
    "        time.sleep(random.uniform(1, 3))  # Random delay between 1 and 3 seconds\n",
    "\n",
    "    return products\n",
    "\n",
    "# Function to get the last page number from the website using the 'Last Page' link\n",
    "def get_last_page_number(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the 'Last Page' link using its aria-label attribute\n",
    "    last_page_link = soup.find('a', attrs={'aria-label': 'Last Page'})\n",
    "    \n",
    "    if last_page_link and 'href' in last_page_link.attrs:\n",
    "        last_page_url = last_page_link['href']\n",
    "        try:\n",
    "            page_number = last_page_url.split('?page=')[1].split('#')[0]\n",
    "            return int(page_number)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting last page number: {e}\")\n",
    "            return 1  # Default to 1 if error occurs\n",
    "    else:\n",
    "        print(\"Last page link not found.\")\n",
    "        return 1  # Default to 1 page if no last page is found\n",
    "\n",
    "# Function to save data to PostgreSQL\n",
    "def save_to_postgresql(products, db_params):\n",
    "    try:\n",
    "        # Connect to your PostgreSQL database\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the database\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO jumia_televisions (product_name, price, old_price, discount, rating, item_id, item_brand, product_url)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        for product in products:\n",
    "            cursor.execute(insert_query, (\n",
    "                product[\"product_name\"],\n",
    "                product[\"price\"],\n",
    "                product[\"old_price\"],\n",
    "                product[\"discount\"],\n",
    "                product[\"rating\"],\n",
    "                product[\"item_id\"],\n",
    "                product[\"item_brand\"],\n",
    "                product[\"product_url\"]\n",
    "            ))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Successfully inserted {len(products)} products into the database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"localhost\",     # Database host\n",
    "    \"database\": \"e-analytics_db\",   # Database name\n",
    "    \"user\": \"postgres\",     # Database user\n",
    "    \"password\": \"password\"  # Database password\n",
    "}\n",
    "\n",
    "# Function to scrape and save data for multiple URLs\n",
    "def scrape_and_save(urls, headers, db_params):\n",
    "    for url in urls:\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        \n",
    "        # Get the last page number for the current URL\n",
    "        last_page = get_last_page_number(url, headers)\n",
    "\n",
    "        # Initialize an empty list to store all products\n",
    "        products_list = []\n",
    "\n",
    "        # Iterate through all pages from 1 to the last page\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "            print(f\"Scraping page {page_num} from {url}...\")\n",
    "\n",
    "            # Scrape the products from the current page\n",
    "            products = scrape_product_details(page_url, headers)\n",
    "            products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "            # Sleep for a random time between requests to avoid overwhelming the server\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # Save the products to PostgreSQL after scraping all pages for this URL\n",
    "        if products_list:\n",
    "            save_to_postgresql(products_list, db_params)\n",
    "            print(f\"Saved {len(products_list)} products from {url} to PostgreSQL.\")\n",
    "        else:\n",
    "            print(f\"No products found on {url}.\")\n",
    "\n",
    "# Define your headers and the URLs to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# List of URLs to scrape (example)\n",
    "urls = [\n",
    "    url1,\n",
    "    url2,\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Start the main scraping process\n",
    "scrape_and_save(urls, headers, db_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the last page number from the website\n",
    "def get_last_page_number(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Try to find the pagination section\n",
    "    pagination = soup.find('ul', class_='pagination')\n",
    "\n",
    "    # If no pagination is found, return 1 (assuming only one page)\n",
    "    if pagination is None:\n",
    "        print(\"No pagination found, assuming only one page.\")\n",
    "        return 1\n",
    "\n",
    "    # If pagination is found, find the last page number\n",
    "    try:\n",
    "        last_page = pagination.find_all('li')[-1].get_text(strip=True)\n",
    "        return int(last_page) if last_page.isdigit() else 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing pagination: {e}\")\n",
    "        return 1\n",
    "\n",
    "# Function to save data to PostgreSQL\n",
    "def save_to_postgresql(products, db_params):\n",
    "    try:\n",
    "        # Connect to your PostgreSQL database\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the database\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO jumia_televisions (product_name, price, old_price, discount, rating, item_id, item_brand, product_url)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        for product in products:\n",
    "            cursor.execute(insert_query, (\n",
    "                product[\"product_name\"],\n",
    "                product[\"price\"],\n",
    "                product[\"old_price\"],\n",
    "                product[\"discount\"],\n",
    "                product[\"rating\"],\n",
    "                product[\"item_id\"],\n",
    "                product[\"item_brand\"],\n",
    "                product[\"product_url\"]\n",
    "            ))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Successfully inserted {len(products)} products into the database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"localhost\",     # Database host\n",
    "    \"database\": \"e-analytics_db\",   # Database name\n",
    "    \"user\": \"postgres\",     # Database user\n",
    "    \"password\": \"password\"  # Database password\n",
    "}\n",
    "\n",
    "# Function to scrape and save data for multiple URLs\n",
    "def scrape_and_save(urls, headers, db_params):\n",
    "    for url in urls:\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        \n",
    "        # Get the last page number for the current URL\n",
    "        last_page = get_last_page_number(url, headers)\n",
    "\n",
    "        # Initialize an empty list to store all products\n",
    "        products_list = []\n",
    "\n",
    "        # Iterate through all pages from 1 to the last page\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "            print(f\"Scraping page {page_num} from {url}...\")\n",
    "\n",
    "            # Scrape the products from the current page\n",
    "            products = scrape_product_details(page_url, headers)\n",
    "            products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "            # Sleep for a random time between requests to avoid overwhelming the server\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # Save the products to PostgreSQL after scraping all pages for this URL\n",
    "        if products_list:\n",
    "            save_to_postgresql(products_list, db_params)\n",
    "            print(f\"Saved {len(products_list)} products from {url} to PostgreSQL.\")\n",
    "        else:\n",
    "            print(f\"No products found on {url}.\")\n",
    "\n",
    "# Define your headers and the URLs to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# List of URLs to scrape (example)\n",
    "urls = [\n",
    "    url1,\n",
    "    url2,\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Start the main scraping process\n",
    "scrape_and_save(urls, headers, db_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import psycopg2\n",
    "\n",
    "# Function to fetch and scrape product details\n",
    "def scrape_product_details(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    catalog_divs = soup.find_all('div', attrs={'data-catalog': 'true'})\n",
    "\n",
    "    products = []\n",
    "    for catalog_div in catalog_divs:\n",
    "        product = catalog_div.find('a', class_='core')\n",
    "        \n",
    "        if product:\n",
    "            name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "            price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "            old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "            discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "            \n",
    "            rating = product.find('div', class_='rev')\n",
    "            if rating:\n",
    "                stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "                reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "            else:\n",
    "                stars = \"N/A\"\n",
    "                reviews_count = \"N/A\"\n",
    "\n",
    "            item_id = product.get('data-gtm-id', \"N/A\")\n",
    "            item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "            product_url = product['href'] if product.has_attr('href') else \"N/A\"\n",
    "\n",
    "            # Store product info in a dictionary\n",
    "            products.append({\n",
    "                \"product_name\": name,\n",
    "                \"price\": price,\n",
    "                \"old_price\": old_price,\n",
    "                \"discount\": discount,\n",
    "                \"rating\": f\"{stars} ({reviews_count} reviews)\",\n",
    "                \"item_id\": item_id,\n",
    "                \"item_brand\": item_brand,\n",
    "                \"product_url\": f\"https://www.jumia.co.ke{product_url}\"\n",
    "            })\n",
    "        \n",
    "        # Optional delay between requests\n",
    "        time.sleep(random.uniform(1, 3))  # Random delay between 1 and 3 seconds\n",
    "\n",
    "    return products\n",
    "\n",
    "# Function to get the last page number from the website\n",
    "def get_last_page_number(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return 1  # Return 1 if failed to fetch page number\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Find the last page number (you might need to adjust the selector based on the website's structure)\n",
    "    pagination = soup.find('ul', class_='pagination')\n",
    "    last_page = pagination.find_all('li')[-1].get_text(strip=True)\n",
    "    return int(last_page) if last_page.isdigit() else 1\n",
    "\n",
    "# Function to save data to PostgreSQL\n",
    "def save_to_postgresql(products, db_params):\n",
    "    try:\n",
    "        # Connect to your PostgreSQL database\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the database\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO jumia_cookers (product_name, price, old_price, discount, rating, item_id, item_brand, product_url)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        for product in products:\n",
    "            cursor.execute(insert_query, (\n",
    "                product[\"product_name\"],\n",
    "                product[\"price\"],\n",
    "                product[\"old_price\"],\n",
    "                product[\"discount\"],\n",
    "                product[\"rating\"],\n",
    "                product[\"item_id\"],\n",
    "                product[\"item_brand\"],\n",
    "                product[\"product_url\"]\n",
    "            ))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Successfully inserted {len(products)} products into the database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"host\": \"localhost\",     # Database host\n",
    "    \"database\": \"e-analytics_db\",   # Database name\n",
    "    \"user\": \"postgres\",     # Database user\n",
    "    \"password\": \"password\"  # Database password\n",
    "}\n",
    "\n",
    "# Function to scrape and save data for multiple URLs\n",
    "def scrape_and_save(urls, headers, db_params):\n",
    "    for url in urls:\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        # Get the last page number for the current URL\n",
    "        last_page = get_last_page_number(url, headers)\n",
    "\n",
    "        # Initialize an empty list to store all products\n",
    "        products_list = []\n",
    "\n",
    "        # Iterate through all pages from 1 to the last page\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "            print(f\"Scraping page {page_num} from {url}...\")\n",
    "\n",
    "            # Scrape the products from the current page\n",
    "            products = scrape_product_details(page_url, headers)\n",
    "            products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "            # Sleep for a random time between requests to avoid overwhelming the server\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # Save the products to PostgreSQL after scraping all pages for this URL\n",
    "        if products_list:\n",
    "            save_to_postgresql(products_list, db_params)\n",
    "            print(f\"Saved {len(products_list)} products from {url} to PostgreSQL.\")\n",
    "        else:\n",
    "            print(f\"No products found on {url}.\")\n",
    "\n",
    "# Define your headers and the URLs to scrape\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# List of URLs to scrape (example)\n",
    "urls = [\n",
    "    \"https://www.jumia.co.ke/home-cooking-appliances-cookers/#catalog-listing\"\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Start the main scraping process\n",
    "scrape_and_save(urls, headers, db_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
