{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (490865904.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    docker run -d \\\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "docker run -d \\\n",
    "  --name postgres-container \\\n",
    "  -e POSTGRES_PASSWORD=yourpassword \\\n",
    "  -e POSTGRES_USER=postgres \\\n",
    "  -e POSTGRES_DB=mydb \\\n",
    "  -p 5432:5432 \\\n",
    "  postgres:17.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d --name my-container -e POSTGRES_PASSWORD=password -e POSTGRES_USER=postgres -e POSTGRES_DB=e-analytics_db -p 5432:5432 postgres:latest\n",
    "\n",
    "docker start my-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate over all URLs to scrape data and save to a CSV file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[43murls\u001b[49m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Get the last page number for the current URL\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     last_page \u001b[38;5;241m=\u001b[39m get_last_page_number(url, headers)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize an empty list to store all products\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate over all URLs to scrape data and save to a CSV file\n",
    "for url in urls:\n",
    "    # Get the last page number for the current URL\n",
    "    last_page = get_last_page_number(url, headers)\n",
    "\n",
    "    # Initialize an empty list to store all products\n",
    "    products_list = []\n",
    "\n",
    "    # Iterate through all pages from 1 to the last page\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        page_url = f\"{url}?page={page_num}#catalog-listing\"\n",
    "        print(f\"Scraping page {page_num} from {url}...\")\n",
    "        \n",
    "        # Scrape the products from the current page\n",
    "        products = scrape_product_details(page_url, headers)\n",
    "        products_list.extend(products)  # Add the scraped products to the main list\n",
    "\n",
    "        # Sleep for a random time between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # Determine the output CSV file name based on the URL\n",
    "    if url == url1:\n",
    "        csv_filename = 'data/scrapped/jumia_scraped_televisions.csv' \n",
    "    else:\n",
    "        csv_filename = 'data/scrapped/jumia_scraped_cookers.csv'\n",
    "\n",
    "    # Save the scraped product details to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[ \n",
    "            'name', 'discounted_price', 'previous_price', 'discount_%', 'id', 'brand', 'rating', 'reviews_count', 'link'\n",
    "        ])\n",
    "        writer.writeheader()  # Write the header row\n",
    "        \n",
    "        for product in products_list:\n",
    "            writer.writerow(product)\n",
    "\n",
    "    print(f\"Scraped {len(products_list)} products from {url} and saved them to '{csv_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape product details from a given URL\n",
    "def scrape_product_details(url, headers):\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = soup.find_all('a', class_='core')\n",
    "\n",
    "    product_details = []\n",
    "\n",
    "    for product in products:\n",
    "        # Extract product link (relative URL)\n",
    "        link = product['href'] if 'href' in product.attrs else None\n",
    "        # Complete the URL if the link is relative\n",
    "        link = f\"https://www.jumia.co.ke{link}\" if link and link.startswith('/') else link\n",
    "    \n",
    "        # Extract product name from 'data-gtm-name' attribute\n",
    "        name = product.get('data-gtm-name', \"N/A\")\n",
    "        \n",
    "        # Extract product price (actual price) from the 'prc' div\n",
    "        price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        old_price = product.find('div', class_='prc')['data-oprc']\n",
    "        discount = product.find('div', class_='bdg _dsct').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "        \n",
    "        # Extract item ID (from data-gtm-id attribute)\n",
    "        item_id = product.get('data-gtm-id', \"N/A\")\n",
    "        \n",
    "        # Extract item brand (from data-gtm-brand attribute)\n",
    "        item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "        \n",
    "        # Extract the stars rating (from the 'stars _m _al' or 'stars _s' class)\n",
    "        stars_rating = product.find('div', class_='stars _m _al') or product.find('div', class_='stars _s')\n",
    "        if stars_rating:\n",
    "            rating = stars_rating.get_text(strip=True).split(\" out of \")[0]  # Extract the rating value (e.g., \"3.9\")\n",
    "        else:\n",
    "            rating = \"N/A\"\n",
    "        \n",
    "        # Extract reviews count (from the 'rev' class or 'verified ratings' link)\n",
    "        #reviews = product.find('div', class_='rev')\n",
    "        #if reviews:\n",
    "        #   reviews_count = reviews.get_text(strip=True).split('(')[-1].split(')')[0]  # Extract the review count (e.g., \"798\")\n",
    "        #else:\n",
    "        #   reviews_link = product.find('a', class_='-plxs _more')\n",
    "        #   reviews_count = reviews_link.get_text(strip=True).split('(')[-1].split(')')[0] if reviews_link else \"N/A\"\n",
    "\n",
    "        # Extract reviews count (from the 'rev' class)\n",
    "        reviews = product.find('div', class_='rev')\n",
    "        if reviews:\n",
    "        # Use regular expression to find the number inside parentheses\n",
    "            reviews_count = re.search(r'\\((\\d+)\\)', reviews.get_text(strip=True))\n",
    "            reviews_count = reviews_count.group(1) if reviews_count else \"N/A\"\n",
    "        else:\n",
    "            reviews_link = product.find('a', class_='-plxs _more')\n",
    "            reviews_count = re.search(r'\\((\\d+)\\)', reviews_link.get_text(strip=True))\n",
    "            reviews_count = reviews_count.group(1) if reviews_count else \"N/A\"\n",
    "\n",
    "        # Store all the extracted product details\n",
    "        product_details.append({\n",
    "            'name': name,\n",
    "            'discounted_price': price,\n",
    "            'previous_price': old_price,\n",
    "            'discount_%': discount,\n",
    "            'id': item_id,\n",
    "            'brand': item_brand,\n",
    "            'rating': rating,\n",
    "            'reviews_count': reviews,\n",
    "            'link': link,\n",
    "        })\n",
    "    \n",
    "    return product_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Function to fetch and scrape product details from a single page\n",
    "def scrape_product_details(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    catalog_divs = soup.find_all('div', attrs={'data-catalog': 'true'})\n",
    "\n",
    "    products = []\n",
    "    for catalog_div in catalog_divs:\n",
    "        product = catalog_div.find('a', class_='core')\n",
    "        \n",
    "        if product:\n",
    "            name = product.find('h3', class_='name').get_text(strip=True) if product.find('h3', class_='name') else \"N/A\"\n",
    "            price = product.find('div', class_='prc').get_text(strip=True) if product.find('div', class_='prc') else \"N/A\"\n",
    "            old_price = product.find('div', class_='old').get_text(strip=True) if product.find('div', class_='old') else \"N/A\"\n",
    "            discount = product.find('div', class_='bdg _dsct _sm').get_text(strip=True) if product.find('div', class_='bdg _dsct _sm') else \"N/A\"\n",
    "            \n",
    "            rating = product.find('div', class_='rev')\n",
    "            if rating:\n",
    "                stars = rating.find('div', class_='stars _s').get_text(strip=True) if rating.find('div', class_='stars _s') else \"N/A\"\n",
    "                reviews_count = rating.get_text(strip=True).split('(')[-1].strip(')') if '(' in rating.get_text() else \"N/A\"\n",
    "            else:\n",
    "                stars = \"N/A\"\n",
    "                reviews_count = \"N/A\"\n",
    "\n",
    "            item_id = product.get('data-gtm-id', \"N/A\")\n",
    "            item_brand = product.get('data-gtm-brand', \"N/A\")\n",
    "            product_url = product['href'] if product.has_attr('href') else \"N/A\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Function to extract old price and discount from a webpage\n",
    "def extract_price_and_discount(url):\n",
    "    # Send HTTP request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    old_price = None\n",
    "    discount = None\n",
    "\n",
    "    # Try to extract data based on the first format\n",
    "    price_div_1 = soup.find('div', class_='-dif -i-ctr')\n",
    "    if price_div_1:\n",
    "        # Get the old price (which is the current price in this case)\n",
    "        current_price = price_div_1.find('span', class_='-tal -gy5 -lthr -fs16 -pvxs -ubpt')\n",
    "        discount_span = price_div_1.find('span', class_='bdg _dsct _dyn -mls')\n",
    "        \n",
    "        if current_price and discount_span:\n",
    "            current_price_text = current_price.get_text().strip()\n",
    "            discount_text = discount_span.get_text().strip()\n",
    "            \n",
    "            # Extract the old price directly (this is the discounted price)\n",
    "            old_price = current_price_text\n",
    "            discount = discount_text\n",
    "\n",
    "    # Try to extract data based on the second format\n",
    "    if not old_price:  # If not found in the first format, check the second format\n",
    "        price_div_2 = soup.find('div', class_='s-prc-w')\n",
    "        if price_div_2:\n",
    "            old_price_tag = price_div_2.find('div', class_='old')\n",
    "            discount_div = price_div_2.find('div', class_='bdg _dsct _sm')\n",
    "\n",
    "            if old_price_tag and discount_div:\n",
    "                old_price = old_price_tag.get_text().strip()\n",
    "                discount = discount_div.get_text().strip()\n",
    "\n",
    "    # Return the results\n",
    "    return old_price, discount\n",
    "\n",
    "# Example URL (replace with the actual URL of the webpage you want to scrape)\n",
    "url = 'https://www.jumia.co.ke/televisions/#catalog-listing'\n",
    "\n",
    "# Extract old price and discount\n",
    "old_price, discount = extract_price_and_discount(url)\n",
    "\n",
    "if old_price and discount:\n",
    "    print(f\"Old Price: {old_price}\")\n",
    "    print(f\"Discount: {discount}\")\n",
    "else:\n",
    "    print(\"Could not extract price and discount.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
