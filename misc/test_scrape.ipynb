{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import logging\n",
    "import re\n",
    "\n",
    "def get_last_page_number(url, headers):\n",
    "    \"\"\"Get the last page number with improved error handling.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Method 1: Try to find the last page number from the pagination section\n",
    "        pagination = soup.find('div', {'class': 'pg-w'})\n",
    "        if pagination:\n",
    "            # Find all page links\n",
    "            page_links = pagination.find_all('a')\n",
    "            if page_links:\n",
    "                # Filter out non-numeric values and get the maximum\n",
    "                page_numbers = []\n",
    "                for link in page_links:\n",
    "                    text = link.get_text().strip()\n",
    "                    try:\n",
    "                        num = int(text)\n",
    "                        page_numbers.append(num)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                \n",
    "                if page_numbers:\n",
    "                    return max(page_numbers)\n",
    "        \n",
    "        # Method 2: Try to find page numbers from the URL pattern\n",
    "        page_pattern = re.compile(r'page=(\\d+)')\n",
    "        matches = page_pattern.findall(response.text)\n",
    "        if matches:\n",
    "            page_numbers = [int(num) for num in matches]\n",
    "            if page_numbers:\n",
    "                return max(page_numbers)\n",
    "        \n",
    "        # Method 3: Count product items and divide by items per page\n",
    "        products = soup.find_all('article', {'class': 'prd'})\n",
    "        if products:\n",
    "            items_per_page = len(products)\n",
    "            total_items_text = soup.find('p', {'class': '-fs14'})\n",
    "            if total_items_text:\n",
    "                total_match = re.search(r'of\\s+(\\d+)', total_items_text.text)\n",
    "                if total_match:\n",
    "                    total_items = int(total_match.group(1))\n",
    "                    return -(-total_items // items_per_page)  # Ceiling division\n",
    "        \n",
    "        # Default to checking next pages until no products found\n",
    "        page = 1\n",
    "        while True:\n",
    "            next_url = f\"{url}?page={page + 1}\"\n",
    "            response = requests.get(next_url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            if not soup.find_all('article', {'class': 'prd'}):\n",
    "                return page\n",
    "            page += 1\n",
    "            if page > 50:  # Safety limit\n",
    "                return 50\n",
    "            time.sleep(1)  # Be nice to the server\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_last_page_number: {str(e)}\")\n",
    "        return 1  # Return 1 as fallback\n",
    "    \n",
    "    return 1  # Default to 1 if all methods fail\n",
    "\n",
    "def scrape_product_details(url, headers):\n",
    "    \"\"\"Scrape product details from a page.\"\"\"\n",
    "    products = []\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all product articles\n",
    "        product_articles = soup.find_all('article', {'class': 'prd'})\n",
    "        \n",
    "        for article in product_articles:\n",
    "            try:\n",
    "                # Extract product info with error handling\n",
    "                name = article.find('h3', {'class': 'name'})\n",
    "                name = name.text.strip() if name else 'N/A'\n",
    "                \n",
    "                price = article.find('div', {'class': 'prc'})\n",
    "                price = price.text.strip() if price else 'N/A'\n",
    "                \n",
    "                brand = article.get('data-brand', 'N/A')\n",
    "                product_url = article.find('a', href=True)\n",
    "                product_url = product_url['href'] if product_url else 'N/A'\n",
    "                \n",
    "                product = {\n",
    "                    'name': name,\n",
    "                    'price': price,\n",
    "                    'brand': brand,\n",
    "                    'url': product_url\n",
    "                }\n",
    "                products.append(product)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error extracting product details: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping page {url}: {str(e)}\")\n",
    "    \n",
    "    return products\n",
    "\n",
    "def main():\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        filename='scraping.log'\n",
    "    )\n",
    "    \n",
    "    # Headers for requests\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # URLs to scrape\n",
    "    url1 = \"https://www.jumia.co.ke/televisions/\"\n",
    "    url2 = \"https://www.jumia.co.ke/home-cooking-appliances-cookers/\"\n",
    "    \n",
    "    # Test pagination for both URLs\n",
    "    for url in [url1, url2]:\n",
    "        try:\n",
    "            logging.info(f\"Testing pagination for {url}\")\n",
    "            last_page = get_last_page_number(url, headers)\n",
    "            logging.info(f\"Last page number for {url}: {last_page}\")\n",
    "            \n",
    "            # Test scraping first page\n",
    "            products = scrape_product_details(url, headers)\n",
    "            logging.info(f\"Found {len(products)} products on first page of {url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
